import streamlit as st
import pandas as pd
import numpy as np
import pickle
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os
import glob
import json
from ast import literal_eval
import tempfile
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import RobustScaler, LabelEncoder
from sklearn.impute import KNNImputer, SimpleImputer
import warnings
import re
from datetime import datetime
warnings.filterwarnings('ignore')

# Configuration de base
st.set_page_config(page_title="NASA Exoplanet Prediction", layout="wide")

# CSS am√©lior√©
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        text-align: center;
        margin-bottom: 2rem;
        color: white;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    .metric-card {
        background: rgba(255,255,255,0.1);
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
        border-left: 4px solid #667eea;
    }
    .prediction-result {
        background: linear-gradient(135deg, #00b09b 0%, #96c93d 100%);
        padding: 1.5rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin: 1rem 0;
    }
    .feedback-section {
        background: rgba(255,255,255,0.05);
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    .tab-content {
        padding: 1rem 0;
    }
    .feature-input {
        margin-bottom: 1rem;
    }
    .feature-group {
        background: rgba(255,255,255,0.05);
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
        border-left: 4px solid #667eea;
    }
    .status-badge {
        display: inline-block;
        padding: 0.25rem 0.75rem;
        border-radius: 15px;
        font-size: 0.8rem;
        font-weight: bold;
        margin: 0.1rem;
    }
    .status-success {
        background-color: #d4edda;
        color: #155724;
        border: 1px solid #c3e6cb;
    }
    .status-warning {
        background-color: #fff3cd;
        color: #856404;
        border: 1px solid #ffeaa7;
    }
    .status-danger {
        background-color: #f8d7da;
        color: #721c24;
        border: 1px solid #f5c6cb;
    }
    .metrics-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
        gap: 0.5rem;
        margin: 1rem 0;
    }
    .metric-item {
        background: rgba(255,255,255,0.05);
        padding: 0.75rem;
        border-radius: 8px;
        text-align: center;
        border-left: 3px solid #667eea;
    }
    .metrics-container {
        background: rgba(255,255,255,0.02);
        padding: 1.5rem;
        border-radius: 10px;
        border: 1px solid rgba(255,255,255,0.1);
        margin: 1rem 0;
    }
    .metric-value {
        font-size: 1.1rem;
        font-weight: bold;
        color: #667eea;
    }
    .metric-label {
        font-size: 0.85rem;
        color: #888;
        margin-bottom: 0.25rem;
    }
    .model-stats-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
        gap: 1rem;
        margin: 1rem 0;
    }
    .stat-card {
        background: rgba(255,255,255,0.05);
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #667eea;
    }
    .hyperparam-info {
        background: rgba(255,255,255,0.02);
        padding: 1rem;
        border-radius: 8px;
        border-left: 3px solid #667eea;
        margin: 0.5rem 0;
        font-size: 0.85rem;
    }
    .batch-prediction-info {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 10px;
        color: white;
        margin: 1rem 0;
    }
    .retrain-info {
        background: linear-gradient(135deg, #00b09b 0%, #96c93d 100%);
        padding: 1.5rem;
        border-radius: 10px;
        color: white;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Noms r√©els des features avec organisation par cat√©gorie
FEATURE_GROUPS = {
    "Detection & Scoring": [
        ("koi_score", "Detection Score [0-1]", 0.0, 1.0, 0.5),
        ("koi_model_snr", "Transit SNR", 0.0, 50.0, 15.0),
        ("habitability_index", "Habitability Index", 0.0, 1.0, 0.3)
    ],
    "Planet Characteristics": [
        ("planet_density_proxy", "Planet Density (proxy) [g/cm¬≥]", 0.0, 10.0, 1.3),
        ("koi_prad", "Planet Radius [Earth radii]", 0.5, 20.0, 2.0),
        ("koi_prad_err1", "Planet Radius Error (+) [Earth radii]", 0.0, 5.0, 0.5)
    ],
    "False Positive Flags": [
        ("koi_fpflag_nt", "FP Flag (Non-Transit) [0/1]", 0, 1, 0),
        ("koi_fpflag_ss", "FP Flag (Stellar Variability) [0/1]", 0, 1, 0),
        ("koi_fpflag_co", "FP Flag (Contamination) [0/1]", 0, 1, 0)
    ],
    "Transit Parameters": [
        ("koi_duration_err1", "Transit Error (+) [hours]", 0.0, 5.0, 0.1),
        ("duration_period_ratio", "Duration/Period Ratio", 0.0, 0.1, 0.01),
        ("koi_time0bk_err1", "Transit Epoch Error (+) [days]", 0.0, 1.0, 0.01),
        ("koi_period", "Orbital Period [days]", 0.5, 500.0, 10.5),
        ("koi_depth", "Transit Depth [ppm]", 0.0, 10000.0, 500.0)
    ],
    "Orbital Parameters": [
        ("koi_impact", "Impact Parameter", 0.0, 1.0, 0.5),
        ("koi_period_err1", "Orbital Period Error (+) [days]", 0.0, 1.0, 0.01)
    ],
    "Stellar Parameters": [
        ("koi_steff_err1", "Stellar Temp Error (-) [K]", 0.0, 500.0, 50.0),
        ("koi_steff_err2", "Stellar Temp Error (+) [K]", 0.0, 500.0, 50.0),
        ("koi_slogg_err2", "log(g) Error (-) [log(cm/s^2)]", 0.0, 1.0, 0.1),
        ("koi_insol", "Insolation Flux [Earth flux]", 0.0, 10000.0, 1.0)
    ]
}

# Mapping des classes
CLASS_MAPPING = {
    0: "Faux Positif",
    1: "Candidat", 
    2: "Exoplan√®te"
}

# Mapping pour l'encodage de koi_disposition
DISPOSITION_MAPPING = {
    'FALSE POSITIVE': 0,
    'CANDIDATE': 1, 
    'CONFIRMED': 2
}

# ================================
# FONCTIONS DE PR√âPROCESSING AVANC√â
# ================================

def apply_advanced_preprocessing(df):
    """
    Applique le pr√©processing avanc√© du code original sur le dataset
    """
    st.info("üîÑ Application du pr√©processing avanc√©...")
    
    # Sauvegarde des donn√©es originales
    df_processed = df.copy()
    
    # ================================
    # 1. NETTOYAGE AVANC√â DES DONN√âES
    # ================================
    st.write("**√âtape 1:** Nettoyage avanc√© des donn√©es")
    
    # Suppression des colonnes avec trop de valeurs manquantes
    missing_threshold = 0.7
    cols_to_drop = []
    
    for col in df_processed.columns:
        missing_pct = df_processed[col].isnull().sum() / len(df_processed)
        if missing_pct > missing_threshold:
            cols_to_drop.append(col)
    
    if cols_to_drop:
        df_processed = df_processed.drop(columns=cols_to_drop)
        st.write(f"üóëÔ∏è {len(cols_to_drop)} colonnes avec >70% de valeurs manquantes supprim√©es")
    
    # Suppression des doublons
    duplicates = df_processed.duplicated().sum()
    if duplicates > 0:
        df_processed = df_processed.drop_duplicates()
        st.write(f"üóëÔ∏è {duplicates} doublons supprim√©s")
    
    # Suppression des colonnes avec une seule valeur unique
    single_value_cols = []
    for col in df_processed.columns:
        if df_processed[col].nunique() <= 1:
            single_value_cols.append(col)
    
    if single_value_cols:
        df_processed = df_processed.drop(columns=single_value_cols)
        st.write(f"üóëÔ∏è Colonnes √† valeur unique supprim√©es: {len(single_value_cols)}")
    
    # ================================
    # 2. IMPUTATION INTELLIGENTE
    # ================================
    st.write("**√âtape 2:** Imputation des valeurs manquantes")
    
    # Identification des types de colonnes
    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()
    
    # Imputation des variables num√©riques avec KNN
    if numeric_cols:
        missing_before = df_processed[numeric_cols].isnull().sum().sum()
        if missing_before > 0:
            knn_imputer = KNNImputer(n_neighbors=5)
            df_processed[numeric_cols] = knn_imputer.fit_transform(df_processed[numeric_cols])
            st.write(f"‚úÖ {missing_before} valeurs num√©riques imput√©es avec KNN")
    
    # Imputation des variables cat√©gorielles
    if categorical_cols:
        for col in categorical_cols:
            if df_processed[col].isnull().sum() > 0:
                mode_value = df_processed[col].mode()[0] if not df_processed[col].mode().empty else 'Unknown'
                df_processed[col] = df_processed[col].fillna(mode_value)
    
    # ================================
    # 3. ENCODAGE DE KOI_DISPOSITION
    # ================================
    st.write("**√âtape 3:** Encodage de la variable cible koi_disposition")
    
    if 'koi_disposition' in df_processed.columns:
        # Encodage de koi_disposition
        le = LabelEncoder()
        df_processed['koi_disposition_encoded'] = le.fit_transform(df_processed['koi_disposition'])
        
        # Sauvegarde du mapping pour interpr√©tation
        target_mapping = dict(zip(le.classes_, le.transform(le.classes_)))
        st.write(f"‚úÖ koi_disposition encod√©e. Mapping: {target_mapping}")
        
        # Suppression de la colonne originale koi_disposition
        df_processed = df_processed.drop(columns=['koi_disposition'])
        st.write("üóëÔ∏è Colonne koi_disposition originale supprim√©e")
    else:
        st.warning("‚ö†Ô∏è Colonne koi_disposition non trouv√©e - v√©rifiez que la variable cible est pr√©sente")
    
    # ================================
    # 4. FEATURE ENGINEERING
    # ================================
    st.write("**√âtape 4:** Feature engineering")
    
    # V√©rification des colonnes n√©cessaires pour le feature engineering
    required_features = ['koi_period', 'koi_prad', 'koi_teq', 'koi_duration']
    available_features = [f for f in required_features if f in df_processed.columns]
    
    if len(available_features) == len(required_features):
        # Cr√©ation de nouvelles features bas√©es sur le domaine astronomique
        # Densit√© relative de la plan√®te
        df_processed['planet_density_proxy'] = df_processed['koi_prad'] / (df_processed['koi_period'] ** (2/3))
        
        # Indice d'habitabilit√© simplifi√©
        df_processed['habitability_index'] = (df_processed['koi_teq'] / 288) * (df_processed['koi_prad'] / 1.0)
        
        # Ratio dur√©e/p√©riode
        df_processed['duration_period_ratio'] = df_processed['koi_duration'] / df_processed['koi_period']
        
        # Binning de variables continues
        df_processed['planet_size_category'] = pd.cut(df_processed['koi_prad'],
                                           bins=[0, 1.25, 2.0, 4.0, float('inf')],
                                           labels=['Earth-like', 'Super-Earth', 'Neptune-like', 'Jupiter-like'])
        
        st.write("‚úÖ Features d'ing√©nierie cr√©√©es: planet_density_proxy, habitability_index, duration_period_ratio, planet_size_category")
    else:
        st.warning(f"‚ö†Ô∏è Features manquantes pour l'ing√©nierie: {set(required_features) - set(available_features)}")
    
    # ================================
    # 5. ENCODAGE DES VARIABLES CAT√âGORIELLES
    # ================================
    st.write("**√âtape 5:** Encodage des variables cat√©gorielles")
    
    categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()
    
    for col in categorical_cols:
        unique_values = df_processed[col].nunique()
        if unique_values <= 10:  # One-hot encoding
            dummies = pd.get_dummies(df_processed[col], prefix=col, drop_first=True)
            df_processed = pd.concat([df_processed, dummies], axis=1)
            df_processed = df_processed.drop(columns=[col])
        else:  # Label encoding
            le = LabelEncoder()
            df_processed[col] = le.fit_transform(df_processed[col].astype(str))
    
    st.success(f"‚úÖ Pr√©processing termin√©. Dimensions finales: {df_processed.shape}")
    
    return df_processed

def validate_and_preprocess_uploaded_data(uploaded_df, base_dataset_features):
    """
    Valide et pr√©traite le dataset upload√© en suivant exactement le m√™me processus
    que le pr√©processing original
    """
    st.info("üîç Validation et pr√©traitement du dataset upload√©...")
    
    # Appliquer le pr√©processing avanc√©
    processed_df = apply_advanced_preprocessing(uploaded_df)
    
    # V√©rifier que toutes les features du base_dataset sont pr√©sentes
    missing_features = [f for f in base_dataset_features if f not in processed_df.columns]
    
    if missing_features:
        st.error(f"‚ùå Features manquantes apr√®s pr√©processing: {missing_features}")
        return None
    
    # S√©lectionner uniquement les features du base_dataset
    final_df = processed_df[base_dataset_features].copy()
    
    st.success(f"‚úÖ Dataset pr√©trait√©: {final_df.shape[0]} √©chantillons, {final_df.shape[1]} features")
    
    return final_df

# FONCTION DE CORRESPONDANCE DES NOMS DE MOD√àLES
def find_matching_model_name(selected_model, comparison_df):
    """Trouve le nom correspondant dans le DataFrame des m√©triques"""
    if comparison_df.empty:
        return None
        
    cleaned_selected = selected_model.lower().replace('_', '').replace('-', '').replace(' ', '')
    
    for model_name_in_df in comparison_df['model_name']:
        cleaned_df_name = model_name_in_df.lower().replace('_', '').replace('-', '').replace(' ', '')
        
        if cleaned_selected == cleaned_df_name:
            return model_name_in_df
            
        if cleaned_selected in cleaned_df_name or cleaned_df_name in cleaned_selected:
            return model_name_in_df
            
        if re.sub(r'[^a-z0-9]', '', cleaned_selected) == re.sub(r'[^a-z0-9]', '', cleaned_df_name):
            return model_name_in_df
    
    return None

def get_model_family(model_name):
    """D√©termine la famille du mod√®le √† partir de son nom"""
    model_name_lower = model_name.lower()
    if 'randomforest' in model_name_lower or 'rf' in model_name_lower:
        return 'RandomForest'
    elif 'xgboost' in model_name_lower or 'xgb' in model_name_lower:
        return 'XGBoost'
    elif 'svm' in model_name_lower:
        return 'SVM'
    elif 'knn' in model_name_lower:
        return 'KNN'
    elif 'logistic' in model_name_lower or 'lr' in model_name_lower:
        return 'LogisticRegression'
    else:
        return 'Autre'

# CHARGEMENT DES DONN√âES
@st.cache_data
def load_model_metrics():
    """Charge les m√©triques des mod√®les depuis le fichier JSON"""
    try:
        with open('all_models_metrics.json', 'r') as f:
            return json.load(f)
    except:
        return []

@st.cache_data
def load_model_comparison():
    """Charge la comparaison des mod√®les depuis le CSV"""
    try:
        df = pd.read_csv('models_comparison.csv')
        df['confusion_matrix'] = df['confusion_matrix'].apply(literal_eval)
        return df
    except Exception as e:
        st.error(f"‚ùå Impossible de charger models_comparison.csv: {e}")
        return pd.DataFrame()

@st.cache_data
def load_dataset():
    """Charge le dataset pour l'entra√Ænement"""
    try:
        df = pd.read_csv('kepler_preprocessed.csv')
        return df
    except Exception as e:
        st.error(f"‚ùå Impossible de charger kepler_preprocessed.csv: {e}")
        return None

@st.cache_data
def load_base_dataset_for_retraining():
    """Charge le dataset de base sans scaling pour le r√©entra√Ænement"""
    try:
        df = pd.read_csv('kepler_before_scaling_selected.csv')
        return df
    except Exception as e:
        st.error(f"‚ùå Impossible de charger kepler_before_scaling_selected.csv: {e}")
        return None

# CHARGEMENT DES MODELES
def load_models_with_dict():
    """Charge les mod√®les depuis des dictionnaires .pkl"""
    pkl_files = [f for f in os.listdir('.') if f.endswith('.pkl')]
    
    models = {}
    model_info = {}
    
    if not pkl_files:
        st.error("‚ùå Aucun fichier .pkl trouv√©")
        return models, model_info
    
    for pkl_file in pkl_files:
        try:
            model_name = pkl_file.replace('.pkl', '')
            
            # Ne pas charger les mod√®les personnalis√©s ou r√©entra√Æn√©s ici, ils seront g√©r√©s s√©par√©ment
            if is_custom_model(model_name) or is_retrained_model(model_name):
                continue
                
            with open(pkl_file, 'rb') as f:
                loaded_data = pickle.load(f)
            
            if isinstance(loaded_data, dict):
                model_object = None
                model_metadata = {}
                
                priority_keys = ['model', 'classifier', 'estimator', 'clf', 'best_estimator_']
                
                for key in priority_keys:
                    if key in loaded_data and hasattr(loaded_data[key], 'predict'):
                        model_object = loaded_data[key]
                        break
                
                if model_object is None:
                    for key, value in loaded_data.items():
                        if hasattr(value, 'predict'):
                            model_object = value
                            break
                
                for key, value in loaded_data.items():
                    model_metadata[key] = {
                        'type': type(value).__name__,
                        'has_predict': hasattr(value, 'predict'),
                        'has_predict_proba': hasattr(value, 'predict_proba') if hasattr(value, 'predict') else False
                    }
                
                models[model_name] = model_object
                model_info[model_name] = {
                    'metadata': model_metadata,
                    'has_model': model_object is not None,
                    'model_type': type(model_object).__name__ if model_object else 'Aucun',
                    'has_predict_proba': hasattr(model_object, 'predict_proba') if model_object else False,
                    'model_family': get_model_family(model_name)
                }
                
            else:
                models[model_name] = loaded_data
                model_info[model_name] = {
                    'metadata': {'direct': type(loaded_data).__name__},
                    'has_model': hasattr(loaded_data, 'predict'),
                    'model_type': type(loaded_data).__name__,
                    'has_predict_proba': hasattr(loaded_data, 'predict_proba'),
                    'model_family': get_model_family(model_name)
                }
            
            if model_info[model_name]['has_model']:
                test_features = np.random.randn(1, 20)
                try:
                    prediction = models[model_name].predict(test_features)
                    if model_info[model_name]['has_predict_proba']:
                        probabilities = models[model_name].predict_proba(test_features)
                    model_info[model_name]['test_success'] = True
                except Exception as e:
                    model_info[model_name]['test_success'] = False
                    model_info[model_name]['test_error'] = str(e)
            
        except Exception as e:
            st.error(f"‚ùå Erreur avec {pkl_file}: {str(e)}")
    
    return models, model_info

# FONCTIONS POUR IDENTIFIER LES TYPES DE MOD√àLES
def is_custom_model(model_name):
    """V√©rifie si un mod√®le est un mod√®le personnalis√©"""
    # Logique plus flexible pour identifier les mod√®les custom
    custom_indicators = ['custom', 'personnalis√©', 'tuning', 'hyperparam']
    model_lower = model_name.lower()
    
    # V√©rifier les indicateurs dans le nom
    for indicator in custom_indicators:
        if indicator in model_lower:
            return True
    
    # V√©rifier s'il est dans les m√©triques custom
    custom_metrics = load_custom_model_metrics()
    for metric in custom_metrics:
        if metric['model_name'] == model_name:
            return True
    
    return False

def is_retrained_model(model_name):
    """V√©rifie si un mod√®le est un mod√®le r√©entra√Æn√©"""
    # Logique plus flexible pour identifier les mod√®les r√©entra√Æn√©s
    retrained_indicators = ['retrained', 'reentraine', 'reentrain√©', 'retrain', 'fine_tuned']
    model_lower = model_name.lower()
    
    # V√©rifier les indicateurs dans le nom
    for indicator in retrained_indicators:
        if indicator in model_lower:
            return True
    
    # V√©rifier s'il est dans les m√©triques retrained
    retrained_metrics = load_retrained_model_metrics()
    for metric in retrained_metrics:
        if metric['model_name'] == model_name:
            return True
    
    return False

# FONCTIONS POUR L'HYPERPARAMETER TUNING AM√âLIOR√âES
def create_custom_model(model_type, hyperparams):
    """Cr√©e un mod√®le personnalis√© avec les hyperparam√®tres sp√©cifi√©s"""
    try:
        if model_type == "RandomForest":
            from sklearn.ensemble import RandomForestClassifier
            model = RandomForestClassifier(
                n_estimators=hyperparams.get('n_estimators', 100),
                max_depth=hyperparams.get('max_depth', None),
                min_samples_split=hyperparams.get('min_samples_split', 2),
                min_samples_leaf=hyperparams.get('min_samples_leaf', 1),
                max_features=hyperparams.get('max_features', 'sqrt'),
                random_state=42,
                n_jobs=-1
            )
        elif model_type == "XGBoost":
            from xgboost import XGBClassifier
            model = XGBClassifier(
                n_estimators=hyperparams.get('n_estimators', 100),
                max_depth=hyperparams.get('max_depth', 3),
                learning_rate=hyperparams.get('learning_rate', 0.1),
                subsample=hyperparams.get('subsample', 1.0),
                colsample_bytree=hyperparams.get('colsample_bytree', 1.0),
                random_state=42,
                n_jobs=-1
            )
        elif model_type == "SVM":
            from sklearn.svm import SVC
            model = SVC(
                C=hyperparams.get('C', 1.0),
                kernel=hyperparams.get('kernel', 'rbf'),
                gamma=hyperparams.get('gamma', 'scale'),
                probability=True,
                random_state=42
            )
        else:
            return None
        
        return model
    except Exception as e:
        st.error(f"Erreur lors de la cr√©ation du mod√®le: {e}")
        return None

def train_and_evaluate_model(model, X_train, X_test, y_train, y_test):
    """Entra√Æne et √©value un mod√®le, retourne les m√©triques"""
    try:
        # Entra√Ænement du mod√®le
        model.fit(X_train, y_train)
        
        # Pr√©dictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None
        
        # Calcul des m√©triques
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)
        cm = confusion_matrix(y_test, y_pred)
        
        # M√©triques d√©taill√©es
        precision = report['weighted avg']['precision']
        recall = report['weighted avg']['recall']
        f1_macro = report['macro avg']['f1-score']
        f1_weighted = report['weighted avg']['f1-score']
        
        # ROC AUC (si probabilit√©s disponibles)
        roc_auc = None
        if y_pred_proba is not None:
            from sklearn.metrics import roc_auc_score
            try:
                roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')
            except:
                roc_auc = 0.5
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_macro': f1_macro,
            'f1_weighted': f1_weighted,
            'roc_auc': roc_auc if roc_auc else 0.5,
            'auc_score': roc_auc if roc_auc else 0.5,
            'confusion_matrix': cm.tolist(),
            'classification_report': report
        }
    except Exception as e:
        st.error(f"Erreur lors de l'entra√Ænement et √©valuation: {e}")
        return None

def load_training_data():
    """Charge les donn√©es d'entra√Ænement depuis kepler_preprocessed.csv"""
    try:
        # Charger le vrai dataset
        df = pd.read_csv('kepler_preprocessed.csv')
        st.success(f"‚úÖ Dataset charg√©: {df.shape[0]} √©chantillons, {df.shape[1]} caract√©ristiques")
        
        # V√©rifier la structure du dataset
        if df.shape[1] < 21:  # 20 features + target
            st.error(f"‚ùå Le dataset doit contenir au moins 21 colonnes (20 features + target). Actuel: {df.shape[1]}")
            return None, None, None, None
        
        # S√©parer features et target
        X = df.iloc[:, :-1].values  # Toutes les colonnes sauf la derni√®re
        y = df.iloc[:, -1].values   # Derni√®re colonne comme target
        
        # V√©rifier les dimensions
        if X.shape[1] != 20:
            st.warning(f"‚ö†Ô∏è Nombre de features diff√©rent de 20. Utilisation des {X.shape[1]} premi√®res colonnes.")
        
        # Split train-test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, 
            test_size=0.2, 
            random_state=42,
            stratify=y  # Pour maintenir la distribution des classes
        )
        
        st.success(f"‚úÖ Split effectu√©: Train={X_train.shape[0]}, Test={X_test.shape[0]}")
        st.info(f"üìä Distribution des classes: {np.unique(y, return_counts=True)}")
        
        return X_train, X_test, y_train, y_test
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement des donn√©es: {e}")
        return None, None, None, None

def save_custom_model_metrics(model_name, metrics, hyperparams):
    """Sauvegarde les m√©triques du mod√®le custom dans un fichier JSON"""
    try:
        # Charger les m√©triques existantes
        custom_metrics_file = 'custom_models_metrics.json'
        if os.path.exists(custom_metrics_file):
            with open(custom_metrics_file, 'r') as f:
                all_custom_metrics = json.load(f)
        else:
            all_custom_metrics = []
        
        # Ajouter les nouvelles m√©triques
        model_metrics = {
            'model_name': model_name,
            'accuracy': metrics['accuracy'],
            'precision': metrics['precision'],
            'recall': metrics['recall'],
            'f1_macro': metrics['f1_macro'],
            'f1_weighted': metrics['f1_weighted'],
            'roc_auc': metrics['roc_auc'],
            'auc_score': metrics['auc_score'],
            'confusion_matrix': metrics['confusion_matrix'],
            'hyperparameters': hyperparams,
            'creation_time': pd.Timestamp.now().isoformat(),
            'model_type': 'Custom'
        }
        
        all_custom_metrics.append(model_metrics)
        
        # Sauvegarder
        with open(custom_metrics_file, 'w') as f:
            json.dump(all_custom_metrics, f, indent=2)
        
        return True
    except Exception as e:
        st.error(f"‚ùå Erreur lors de la sauvegarde des m√©triques: {e}")
        return False

def load_custom_model_metrics():
    """Charge les m√©triques des mod√®les custom"""
    try:
        custom_metrics_file = 'custom_models_metrics.json'
        if os.path.exists(custom_metrics_file):
            with open(custom_metrics_file, 'r') as f:
                return json.load(f)
        return []
    except:
        return []

def delete_custom_model(model_name):
    """Supprime un mod√®le personnalis√© et toutes ses donn√©es"""
    try:
        # Supprimer le fichier .pkl
        pkl_file = f"{model_name}.pkl"
        if os.path.exists(pkl_file):
            os.remove(pkl_file)
            st.write(f"üóëÔ∏è Fichier mod√®le supprim√©: {pkl_file}")
        
        # Supprimer les m√©triques du fichier JSON
        custom_metrics_file = 'custom_models_metrics.json'
        if os.path.exists(custom_metrics_file):
            with open(custom_metrics_file, 'r') as f:
                all_custom_metrics = json.load(f)
            
            # Filtrer pour garder seulement les autres mod√®les
            updated_metrics = [m for m in all_custom_metrics if m['model_name'] != model_name]
            
            with open(custom_metrics_file, 'w') as f:
                json.dump(updated_metrics, f, indent=2)
            
            st.write(f"üóëÔ∏è M√©triques supprim√©es pour: {model_name}")
        
        # Supprimer du session_state
        if model_name in st.session_state.custom_models:
            del st.session_state.custom_models[model_name]
        
        return True
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors de la suppression du mod√®le {model_name}: {e}")
        return False

def delete_all_custom_models():
    """Supprime tous les mod√®les personnalis√©s"""
    try:
        # Identifier tous les mod√®les custom
        custom_models_to_delete = []
        for model_name in list(st.session_state.custom_models.keys()):
            if is_custom_model(model_name):
                custom_models_to_delete.append(model_name)
        
        # Supprimer chaque mod√®le custom
        for model_name in custom_models_to_delete:
            delete_custom_model(model_name)
        
        st.success(f"‚úÖ Tous les mod√®les personnalis√©s ont √©t√© supprim√©s ({len(custom_models_to_delete)} mod√®les)")
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors de la suppression: {e}")

# FONCTIONS POUR LE REENTRAINEMENT AVEC PR√âPROCESSING AVANC√â
def validate_dataset_for_retraining(uploaded_df, base_dataset):
    """Valide le dataset upload√© pour le r√©entra√Ænement"""
    try:
        # V√©rifier que les features n√©cessaires pour le feature engineering sont pr√©sentes
        required_engineering_features = ['koi_period', 'koi_prad', 'koi_teq', 'koi_duration']
        missing_engineering_features = [f for f in required_engineering_features if f not in uploaded_df.columns]
        
        if missing_engineering_features:
            st.warning(f"‚ö†Ô∏è Features manquantes pour l'ing√©nierie: {missing_engineering_features}")
            st.info("Le feature engineering sera limit√© sans ces features")
        
        # V√©rifier si koi_disposition est pr√©sente
        if 'koi_disposition' not in uploaded_df.columns:
            st.warning("‚ö†Ô∏è Colonne koi_disposition non trouv√©e - v√©rifiez que la variable cible est pr√©sente")
        
        # V√©rifier les dimensions
        if uploaded_df.shape[1] < 10:  # Minimum raisonnable
            st.error(f"Le dataset doit contenir au moins 10 colonnes")
            return False
        
        if uploaded_df.isnull().sum().sum() > 0:
            st.warning("Le dataset contient des valeurs manquantes - elles seront trait√©es lors du pr√©traitement")
        
        return True
    except Exception as e:
        st.error(f"‚ùå Erreur lors de la validation du dataset: {e}")
        return False

def preprocess_uploaded_dataset(uploaded_df, base_dataset):
    """Pr√©traite le dataset upload√© en utilisant les m√™mes √©tapes que le preprocessing original"""
    try:
        st.info("üîÑ D√©but du pr√©traitement avanc√© du dataset upload√©...")
        
        # Appliquer le pr√©processing avanc√©
        processed_df = apply_advanced_preprocessing(uploaded_df)
        
        # V√©rifier que toutes les features du base_dataset sont pr√©sentes
        base_features = base_dataset.columns.tolist()
        missing_features = [feature for feature in base_features if feature not in processed_df.columns]
        
        if missing_features:
            st.error(f"‚ùå Features manquantes apr√®s pr√©processing: {missing_features}")
            return None, None
        
        # S√©lectionner uniquement les colonnes pr√©sentes dans le base_dataset
        df_final = processed_df[base_features].copy()
        
        st.success(f"‚úÖ S√©lection des features: {len(base_features)} colonnes conserv√©es")
        
        # Statistiques de pr√©traitement
        preprocessing_stats = {
            'original_samples': len(uploaded_df),
            'final_samples': len(df_final),
            'original_features': len(uploaded_df.columns),
            'final_features': len(df_final.columns)
        }
        
        return df_final, preprocessing_stats
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors du pr√©traitement: {e}")
        return None, None

def merge_and_clean_datasets(base_df, new_df, remove_duplicates=True, remove_na=True):
    """Fusionne et nettoie les datasets"""
    try:
        # V√©rifier que les datasets ont la m√™me structure
        if base_df.shape[1] != new_df.shape[1]:
            st.error(f"Les datasets ont des structures diff√©rentes: {base_df.shape[1]} vs {new_df.shape[1]} colonnes")
            return None, None
        
        # Fusionner les datasets
        merged_data = pd.concat([base_df, new_df], ignore_index=True)
        original_size = len(merged_data)
        
        stats = {
            'original_size': original_size,
            'duplicates_removed': 0,
            'na_removed': 0,
            'final_size': 0
        }
        
        # Supprimer les doublons
        if remove_duplicates:
            before_dedup = len(merged_data)
            merged_data = merged_data.drop_duplicates()
            stats['duplicates_removed'] = before_dedup - len(merged_data)
        
        # Supprimer les valeurs manquantes
        if remove_na:
            before_na = len(merged_data)
            merged_data = merged_data.dropna()
            stats['na_removed'] = before_na - len(merged_data)
        
        stats['final_size'] = len(merged_data)
        
        return merged_data, stats
        
    except Exception as e:
        st.error(f"Erreur lors de la fusion des datasets: {e}")
        return None, None

def apply_robust_scaling(merged_data):
    """Applique le RobustScaler comme dans le preprocessing original"""
    try:
        # Identifier les colonnes num√©riques
        numeric_cols = merged_data.select_dtypes(include=[np.number]).columns.tolist()
        
        # Exclure la variable cible (suppos√©e √™tre la derni√®re colonne)
        target_col = merged_data.columns[-1]
        numeric_cols_to_scale = [col for col in numeric_cols if col != target_col]
        
        # Appliquer RobustScaler
        scaler = RobustScaler()
        scaled_data = merged_data.copy()
        scaled_data[numeric_cols_to_scale] = scaler.fit_transform(merged_data[numeric_cols_to_scale])
        
        st.success(f"‚úÖ Scaling appliqu√© avec RobustScaler sur {len(numeric_cols_to_scale)} features")
        return scaled_data
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors du scaling: {e}")
        return merged_data

def retrain_model_on_merged_data(model, merged_data):
    """R√©entra√Æne un mod√®le sur les donn√©es fusionn√©es"""
    try:
        # S√©parer features et target
        X = merged_data.iloc[:, :-1].values
        y = merged_data.iloc[:, -1].values
        
        # Split train-test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Entra√Ænement du mod√®le
        model.fit(X_train, y_train)
        
        # √âvaluation
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        
        # M√©triques compl√®tes
        metrics = {
            'accuracy': accuracy,
            'precision': precision_score(y_test, y_pred, average='weighted'),
            'recall': recall_score(y_test, y_pred, average='weighted'),
            'f1_macro': f1_score(y_test, y_pred, average='macro'),
            'f1_weighted': f1_score(y_test, y_pred, average='weighted'),
            'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),
            'classification_report': classification_report(y_test, y_pred, output_dict=True)
        }
        
        # ROC AUC si disponible
        if hasattr(model, 'predict_proba'):
            try:
                y_proba = model.predict_proba(X_test)
                metrics['roc_auc'] = roc_auc_score(y_test, y_proba, multi_class='ovr')
                metrics['auc_score'] = metrics['roc_auc']
            except:
                metrics['roc_auc'] = 0.5
                metrics['auc_score'] = 0.5
        
        return model, metrics, None
        
    except Exception as e:
        return model, None, f"Erreur lors du r√©entra√Ænement: {e}"

def save_retrained_model_metrics(model_name, metrics, original_model_name, dataset_stats):
    """Sauvegarde les m√©triques du mod√®le r√©entra√Æn√©"""
    try:
        retrained_metrics_file = 'retrained_models_metrics.json'
        
        # Charger les m√©triques existantes
        if os.path.exists(retrained_metrics_file):
            with open(retrained_metrics_file, 'r') as f:
                all_metrics = json.load(f)
        else:
            all_metrics = []
        
        # Ajouter les nouvelles m√©triques
        model_metrics = {
            'model_name': model_name,
            'original_model': original_model_name,
            'accuracy': metrics['accuracy'],
            'precision': metrics['precision'],
            'recall': metrics['recall'],
            'f1_macro': metrics['f1_macro'],
            'f1_weighted': metrics['f1_weighted'],
            'roc_auc': metrics.get('roc_auc', 0.5),
            'auc_score': metrics.get('auc_score', 0.5),
            'confusion_matrix': metrics['confusion_matrix'],
            'dataset_stats': dataset_stats,
            'retrain_time': pd.Timestamp.now().isoformat(),
            'model_type': 'Retrained'
        }
        
        all_metrics.append(model_metrics)
        
        # Sauvegarder
        with open(retrained_metrics_file, 'w') as f:
            json.dump(all_metrics, f, indent=2)
        
        return True
    except Exception as e:
        st.error(f"‚ùå Erreur lors de la sauvegarde des m√©triques: {e}")
        return False

def load_retrained_model_metrics():
    """Charge les m√©triques des mod√®les r√©entra√Æn√©s"""
    try:
        retrained_metrics_file = 'retrained_models_metrics.json'
        if os.path.exists(retrained_metrics_file):
            with open(retrained_metrics_file, 'r') as f:
                return json.load(f)
        return []
    except:
        return []

def delete_retrained_model(model_name):
    """Supprime un mod√®le r√©entra√Æn√© et toutes ses donn√©es"""
    try:
        # Supprimer le fichier .pkl
        pkl_file = f"{model_name}.pkl"
        if os.path.exists(pkl_file):
            os.remove(pkl_file)
            st.write(f"üóëÔ∏è Fichier mod√®le supprim√©: {pkl_file}")
        
        # Supprimer les m√©triques du fichier JSON
        retrained_metrics_file = 'retrained_models_metrics.json'
        if os.path.exists(retrained_metrics_file):
            with open(retrained_metrics_file, 'r') as f:
                all_retrained_metrics = json.load(f)
            
            # Filtrer pour garder seulement les autres mod√®les
            updated_metrics = [m for m in all_retrained_metrics if m['model_name'] != model_name]
            
            with open(retrained_metrics_file, 'w') as f:
                json.dump(updated_metrics, f, indent=2)
            
            st.write(f"üóëÔ∏è M√©triques supprim√©es pour: {model_name}")
        
        # Supprimer du session_state
        if model_name in st.session_state.custom_models:
            del st.session_state.custom_models[model_name]
        
        return True
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors de la suppression du mod√®le {model_name}: {e}")
        return False

def delete_all_retrained_models():
    """Supprime tous les mod√®les r√©entra√Æn√©s"""
    try:
        # Identifier tous les mod√®les r√©entra√Æn√©s
        retrained_models_to_delete = []
        for model_name in list(st.session_state.custom_models.keys()):
            if is_retrained_model(model_name):
                retrained_models_to_delete.append(model_name)
        
        # Supprimer chaque mod√®le r√©entra√Æn√©
        for model_name in retrained_models_to_delete:
            delete_retrained_model(model_name)
        
        st.success(f"‚úÖ Tous les mod√®les r√©entra√Æn√©s ont √©t√© supprim√©s ({len(retrained_models_to_delete)} mod√®les)")
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors de la suppression: {e}")

def get_retrained_models_count():
    """Retourne le nombre de mod√®les r√©entra√Æn√©s"""
    count = 0
    for model_name in st.session_state.custom_models.keys():
        if is_retrained_model(model_name):
            count += 1
    return count

def get_custom_models_count():
    """Retourne le nombre de mod√®les personnalis√©s"""
    count = 0
    for model_name in st.session_state.custom_models.keys():
        if is_custom_model(model_name):
            count += 1
    return count

# FONCTIONS D'AFFICHAGE
def display_all_model_metrics_real_time(selected_model, comparison_df, model_info):
    """Affiche TOUTES les m√©triques du mod√®le s√©lectionn√© en temps r√©el"""
    matching_model_name = find_matching_model_name(selected_model, comparison_df)
    
    # V√©rifier si c'est un mod√®le r√©entra√Æn√©
    if is_retrained_model(selected_model):
        retrained_metrics = load_retrained_model_metrics()
        retrained_model_data = None
        for metric in retrained_metrics:
            if metric['model_name'] == selected_model:
                retrained_model_data = metric
                break
        
        if retrained_model_data:
            display_retrained_metrics_from_json(retrained_model_data, selected_model)
        else:
            st.warning(f"‚ÑπÔ∏è Aucune m√©trique disponible pour le mod√®le r√©entra√Æn√© '{selected_model}'")
        return
    
    # V√©rifier si c'est un mod√®le custom
    if is_custom_model(selected_model):
        custom_metrics = load_custom_model_metrics()
        custom_model_data = None
        for metric in custom_metrics:
            if metric['model_name'] == selected_model:
                custom_model_data = metric
                break
        
        if custom_model_data:
            display_custom_metrics_from_json(custom_model_data, selected_model)
        else:
            st.warning(f"‚ÑπÔ∏è Aucune m√©trique disponible pour le mod√®le personnalis√© '{selected_model}'")
        return
    
    # Sinon, c'est un mod√®le de base
    if not matching_model_name:
        st.warning(f"‚ÑπÔ∏è Aucune m√©trique disponible pour le mod√®le '{selected_model}'")
        return
    
    # Afficher les m√©triques du mod√®le de base
    model_data = comparison_df[comparison_df['model_name'] == matching_model_name].iloc[0]
    display_base_model_metrics(model_data, matching_model_name, model_info)

def display_retrained_metrics_from_json(model_data, model_name):
    """Affiche les m√©triques d'un mod√®le r√©entra√Æn√© depuis le JSON"""
    st.markdown(f"### üìä M√©triques Compl√®tes du Mod√®le - {model_name}")
    
    with st.container():
        st.markdown('<div class="metrics-container">', unsafe_allow_html=True)
        
        # En-t√™te avec informations de r√©entra√Ænement
        st.info(f"üîÑ Mod√®le r√©entra√Æn√© √† partir de: **{model_data['original_model']}**")
        st.write(f"**Date de r√©entra√Ænement:** {pd.to_datetime(model_data['retrain_time']).strftime('%Y-%m-%d %H:%M')}")
        
        # 1. M√âTRIQUES PRINCIPALES
        st.subheader("üéØ M√©triques Principales")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Accuracy", f"{model_data['accuracy']:.4f}")
            st.metric("Precision", f"{model_data['precision']:.4f}")
        with col2:
            st.metric("Recall", f"{model_data['recall']:.4f}")
            st.metric("F1 Macro", f"{model_data['f1_macro']:.4f}")
        with col3:
            st.metric("F1 Weighted", f"{model_data['f1_weighted']:.4f}")
            st.metric("ROC AUC", f"{model_data.get('roc_auc', 0):.4f}")
        with col4:
            st.metric("AUC Score", f"{model_data.get('auc_score', 0):.4f}")
            st.metric("Type", "R√©entra√Æn√©")
        
        # 2. STATISTIQUES DU DATASET
        st.markdown("---")
        st.subheader("üìà Statistiques du Dataset")
        
        dataset_stats = model_data.get('dataset_stats', {})
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Taille originale", f"{dataset_stats.get('original_size', 0)}")
        with col2:
            st.metric("Doublons supprim√©s", f"{dataset_stats.get('duplicates_removed', 0)}")
        with col3:
            st.metric("NA supprim√©s", f"{dataset_stats.get('na_removed', 0)}")
        with col4:
            st.metric("Taille finale", f"{dataset_stats.get('final_size', 0)}")
        
        # 3. MATRICE DE CONFUSION
        st.markdown("---")
        st.subheader("üéØ Matrice de Confusion")
        
        cm = np.array(model_data['confusion_matrix'])
        
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                    xticklabels=["Faux Positif", "Candidat", "Exoplan√®te"],
                    yticklabels=["Faux Positif", "Candidat", "Exoplan√®te"])
        ax.set_xlabel('Pr√©dit')
        ax.set_ylabel('R√©el')
        ax.set_title(f'Matrice de Confusion - {model_name}')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        st.pyplot(fig)
        
        st.markdown('</div>', unsafe_allow_html=True)

def display_custom_metrics_from_json(model_data, model_name):
    """Affiche les m√©triques d'un mod√®le custom depuis le JSON"""
    st.markdown(f"### üìä M√©triques Compl√®tes du Mod√®le - {model_name}")
    
    with st.container():
        st.markdown('<div class="metrics-container">', unsafe_allow_html=True)
        
        # 1. M√âTRIQUES PRINCIPALES
        st.subheader("üéØ M√©triques Principales")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Accuracy", f"{model_data['accuracy']:.4f}")
            st.metric("Precision", f"{model_data['precision']:.4f}")
        with col2:
            st.metric("Recall", f"{model_data['recall']:.4f}")
            st.metric("F1 Macro", f"{model_data['f1_macro']:.4f}")
        with col3:
            st.metric("F1 Weighted", f"{model_data['f1_weighted']:.4f}")
            st.metric("ROC AUC", f"{model_data['roc_auc']:.4f}")
        with col4:
            st.metric("AUC Score", f"{model_data['auc_score']:.4f}")
            st.metric("Type", "Personnalis√©")
        
        # 2. STATISTIQUES D√âTAILL√âES
        st.markdown("---")
        st.subheader("üìà Analyse de la Matrice de Confusion")
        
        cm = np.array(model_data['confusion_matrix'])
        
        total_samples = np.sum(cm)
        true_positives = np.diag(cm)
        false_positives = np.sum(cm, axis=0) - true_positives
        false_negatives = np.sum(cm, axis=1) - true_positives
        
        precision_per_class = true_positives / (true_positives + false_positives)
        recall_per_class = true_positives / (true_positives + false_negatives)
        f1_per_class = 2 * (precision_per_class * recall_per_class) / (precision_per_class + recall_per_class)
        
        classes = ["Faux Positif", "Candidat", "Exoplan√®te"]
        
        col1, col2, col3 = st.columns(3)
        
        for i, class_name in enumerate(classes):
            if i == 0:
                with col1:
                    st.markdown(f"**{class_name}**")
                    st.metric("Pr√©cision", f"{precision_per_class[i]:.3f}")
                    st.metric("Rappel", f"{recall_per_class[i]:.3f}")
                    st.metric("F1-Score", f"{f1_per_class[i]:.3f}")
            elif i == 1:
                with col2:
                    st.markdown(f"**{class_name}**")
                    st.metric("Pr√©cision", f"{precision_per_class[i]:.3f}")
                    st.metric("Rappel", f"{recall_per_class[i]:.3f}")
                    st.metric("F1-Score", f"{f1_per_class[i]:.3f}")
            else:
                with col3:
                    st.markdown(f"**{class_name}**")
                    st.metric("Pr√©cision", f"{precision_per_class[i]:.3f}")
                    st.metric("Rappel", f"{recall_per_class[i]:.3f}")
                    st.metric("F1-Score", f"{f1_per_class[i]:.3f}")
        
        # 3. MATRICE DE CONFUSION VISUELLE
        st.markdown("---")
        st.subheader("üéØ Matrice de Confusion")
        
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                    xticklabels=classes,
                    yticklabels=classes)
        ax.set_xlabel('Pr√©dit')
        ax.set_ylabel('R√©el')
        ax.set_title(f'Matrice de Confusion - {model_name}')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        st.pyplot(fig)
        
        # 4. HYPERPARAM√àTRES
        st.markdown("---")
        st.subheader("‚öôÔ∏è Hyperparam√®tres du Mod√®le")
        st.json(model_data['hyperparameters'])
        
        st.markdown('</div>', unsafe_allow_html=True)

def display_base_model_metrics(model_data, model_name, model_info):
    """Affiche les m√©triques d'un mod√®le de base"""
    st.markdown(f"### üìä M√©triques Compl√®tes du Mod√®le - {model_name}")
    
    with st.container():
        st.markdown('<div class="metrics-container">', unsafe_allow_html=True)
        
        # 1. M√âTRIQUES PRINCIPALES
        st.subheader("üéØ M√©triques Principales")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Accuracy", f"{model_data['accuracy']:.4f}")
            st.metric("Precision", f"{model_data['precision']:.4f}")
        with col2:
            st.metric("Recall", f"{model_data['recall']:.4f}")
            st.metric("F1 Macro", f"{model_data['f1_macro']:.4f}")
        with col3:
            st.metric("F1 Weighted", f"{model_data['f1_weighted']:.4f}")
            st.metric("ROC AUC", f"{model_data['roc_auc']:.4f}")
        with col4:
            st.metric("AUC Score", f"{model_data['auc_score']:.4f}")
            st.metric("Rang", f"{model_data['rank']:.0f}")
        
        # 2. STATISTIQUES D√âTAILL√âES
        st.markdown("---")
        st.subheader("üìà Analyse de la Matrice de Confusion")
        
        cm = np.array(model_data['confusion_matrix'])
        
        total_samples = np.sum(cm)
        true_positives = np.diag(cm)
        false_positives = np.sum(cm, axis=0) - true_positives
        false_negatives = np.sum(cm, axis=1) - true_positives
        
        precision_per_class = true_positives / (true_positives + false_positives)
        recall_per_class = true_positives / (true_positives + false_negatives)
        f1_per_class = 2 * (precision_per_class * recall_per_class) / (precision_per_class + recall_per_class)
        
        classes = ["Faux Positif", "Candidat", "Exoplan√®te"]
        
        col1, col2, col3 = st.columns(3)
        
        for i, class_name in enumerate(classes):
            if i == 0:
                with col1:
                    st.markdown(f"**{class_name}**")
                    st.metric("Pr√©cision", f"{precision_per_class[i]:.3f}")
                    st.metric("Rappel", f"{recall_per_class[i]:.3f}")
                    st.metric("F1-Score", f"{f1_per_class[i]:.3f}")
            elif i == 1:
                with col2:
                    st.markdown(f"**{class_name}**")
                    st.metric("Pr√©cision", f"{precision_per_class[i]:.3f}")
                    st.metric("Rappel", f"{recall_per_class[i]:.3f}")
                    st.metric("F1-Score", f"{f1_per_class[i]:.3f}")
            else:
                with col3:
                    st.markdown(f"**{class_name}**")
                    st.metric("Pr√©cision", f"{precision_per_class[i]:.3f}")
                    st.metric("Rappel", f"{recall_per_class[i]:.3f}")
                    st.metric("F1-Score", f"{f1_per_class[i]:.3f}")
        
        # 3. MATRICE DE CONFUSION VISUELLE
        st.markdown("---")
        st.subheader("üéØ Matrice de Confusion")
        
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                    xticklabels=classes,
                    yticklabels=classes)
        ax.set_xlabel('Pr√©dit')
        ax.set_ylabel('R√©el')
        ax.set_title(f'Matrice de Confusion - {model_name}')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        st.pyplot(fig)
        
        # 4. HYPERPARAM√àTRES
        st.markdown("---")
        st.subheader("‚öôÔ∏è Hyperparam√®tres du Mod√®le")
        
        if isinstance(model_data['hyperparameters'], str):
            try:
                hyperparams = literal_eval(model_data['hyperparameters'])
                st.json(hyperparams)
            except:
                st.write(model_data['hyperparameters'])
        else:
            st.json(model_data['hyperparameters'])
        
        st.markdown('</div>', unsafe_allow_html=True)

def display_model_comparison(df):
    """Affiche la comparaison des mod√®les"""
    st.subheader("üìä Comparaison des Mod√®les")
    
    display_cols = ['model_name', 'accuracy', 'precision', 'recall', 'f1_macro', 'f1_weighted', 'roc_auc', 'auc_score', 'rank']
    display_df = df[display_cols].copy()
    
    for col in ['accuracy', 'precision', 'recall', 'f1_macro', 'f1_weighted', 'roc_auc', 'auc_score']:
        display_df[col] = display_df[col].apply(lambda x: f"{x:.4f}")
    
    st.dataframe(display_df, use_container_width=True)
    
    st.subheader("üìà Graphiques de Comparaison")
    
    col1, col2 = st.columns(2)
    
    with col1:
        fig, ax = plt.subplots(figsize=(10, 6))
        models_plot = df.head(10)
        ax.barh(models_plot['model_name'], models_plot['accuracy'])
        ax.set_xlabel('Accuracy')
        ax.set_title('Comparaison de l\'Accuracy par Mod√®le')
        plt.tight_layout()
        st.pyplot(fig)
    
    with col2:
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.barh(models_plot['model_name'], models_plot['f1_macro'])
        ax.set_xlabel('F1 Macro Score')
        ax.set_title('Comparaison du F1-Score par Mod√®le')
        plt.tight_layout()
        st.pyplot(fig)

def display_model_metrics(selected_model, df):
    """Affiche les m√©triques d√©taill√©es d'un mod√®le"""
    matching_model_name = find_matching_model_name(selected_model, df)
    
    if not matching_model_name:
        st.warning(f"Donn√©es non disponibles pour le mod√®le '{selected_model}'")
        return
    
    st.subheader(f"üìä M√©triques D√©taill√©es - {matching_model_name}")
    
    model_data = df[df['model_name'] == matching_model_name].iloc[0]
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Accuracy", f"{model_data['accuracy']:.4f}")
        st.metric("Precision", f"{model_data['precision']:.4f}")
    with col2:
        st.metric("Recall", f"{model_data['recall']:.4f}")
        st.metric("F1 Macro", f"{model_data['f1_macro']:.4f}")
    with col3:
        st.metric("F1 Weighted", f"{model_data['f1_weighted']:.4f}")
        st.metric("ROC AUC", f"{model_data['roc_auc']:.4f}")
    with col4:
        st.metric("AUC Score", f"{model_data['auc_score']:.4f}")
        st.metric("Rang", f"{model_data['rank']:.0f}")
    
    st.subheader("üìà Matrice de Confusion")
    cm = np.array(model_data['confusion_matrix'])
    
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=["Faux Positif", "Candidat", "Exoplan√®te"],
                yticklabels=["Faux Positif", "Candidat", "Exoplan√®te"])
    ax.set_xlabel('Pr√©dit')
    ax.set_ylabel('R√©el')
    ax.set_title(f'Matrice de Confusion - {matching_model_name}')
    plt.xticks(rotation=45)
    plt.yticks(rotation=0)
    st.pyplot(fig)
    
    st.subheader("‚öôÔ∏è Hyperparam√®tres")
    if isinstance(model_data['hyperparameters'], str):
        try:
            hyperparams = literal_eval(model_data['hyperparameters'])
            st.json(hyperparams)
        except:
            st.write(model_data['hyperparameters'])
    else:
        st.json(model_data['hyperparameters'])

# FONCTIONS POUR LA GESTION DES FEATURES
def initialize_features():
    """Initialise les valeurs des features dans session_state"""
    if 'feature_values' not in st.session_state:
        default_values = {}
        for group_name, features in FEATURE_GROUPS.items():
            for feature_key, feature_name, min_val, max_val, default_val in features:
                default_values[feature_key] = default_val
        st.session_state.feature_values = default_values

def get_feature_description(feature_name):
    """Retourne la description d'une feature"""
    descriptions = {
        "koi_score": "Score de d√©tection de l'exoplan√®te (0-1)",
        "koi_model_snr": "Rapport signal/bruit du transit",
        "habitability_index": "Indice d'habitabilit√© de la plan√®te",
        "planet_density_proxy": "Densit√© estim√©e de la plan√®te",
        "koi_prad": "Rayon de la plan√®te en rayons terrestres",
        "koi_prad_err1": "Erreur sur le rayon de la plan√®te",
        "koi_fpflag_nt": "Indicateur de faux positif non li√© au transit",
        "koi_fpflag_ss": "Indicateur de faux positif d√ª √† la variabilit√© stellaire",
        "koi_fpflag_co": "Indicateur de faux positif d√ª √† la contamination",
        "koi_duration_err1": "Erreur sur la dur√©e du transit",
        "duration_period_ratio": "Ratio dur√©e du transit / p√©riode orbitale",
        "koi_time0bk_err1": "Erreur sur l'√©poque du transit",
        "koi_period": "P√©riode orbitale en jours",
        "koi_depth": "Profondeur du transit en parties par million",
        "koi_impact": "Param√®tre d'impact du transit",
        "koi_period_err1": "Erreur sur la p√©riode orbitale",
        "koi_steff_err1": "Erreur n√©gative sur la temp√©rature stellaire",
        "koi_steff_err2": "Erreur positive sur la temp√©rature stellaire",
        "koi_slogg_err2": "Erreur sur la gravit√© de surface stellaire",
        "koi_insol": "Flux d'insolation en unit√©s terrestres"
    }
    return descriptions.get(feature_name, "Caract√©ristique d'exoplan√®te")

# FONCTIONS POUR LA PR√âDICTION PAR LOT AM√âLIOR√âE
def validate_batch_csv(uploaded_df):
    """Valide le fichier CSV pour la pr√©diction par lot"""
    try:
        # V√©rifier le nombre de colonnes
        if uploaded_df.shape[1] < 20:
            return False, f"Le fichier doit contenir au moins 20 colonnes de caract√©ristiques. Actuel: {uploaded_df.shape[1]}"
        
        # V√©rifier les valeurs manquantes
        if uploaded_df.isnull().any().any():
            missing_cols = uploaded_df.columns[uploaded_df.isnull().any()].tolist()
            return False, f"Valeurs manquantes d√©tect√©es dans les colonnes: {missing_cols}"
        
        # V√©rifier les types de donn√©es (doivent √™tre num√©riques)
        for col in uploaded_df.columns[:20]:  # V√©rifier les 20 premi√®res colonnes
            if not pd.api.types.is_numeric_dtype(uploaded_df[col]):
                return False, f"La colonne '{col}' doit contenir des valeurs num√©riques"
        
        return True, "Fichier valide"
    
    except Exception as e:
        return False, f"Erreur lors de la validation: {str(e)}"

def process_batch_prediction(model, batch_df, model_name):
    """Traite la pr√©diction par lot et g√©n√®re les r√©sultats"""
    try:
        # Extraire les features (20 premi√®res colonnes)
        batch_features = batch_df.iloc[:, :20].values
        
        # Faire les pr√©dictions
        predictions_numeric = model.predict(batch_features)
        predictions_text = [CLASS_MAPPING.get(pred, "Inconnu") for pred in predictions_numeric]
        
        # Cr√©er le DataFrame de r√©sultats
        result_df = batch_df.copy()
        result_df['Prediction_Num√©rique'] = predictions_numeric
        result_df['Prediction_Classe'] = predictions_text
        
        # Ajouter les probabilit√©s si disponibles
        if hasattr(model, 'predict_proba'):
            try:
                probabilities = model.predict_proba(batch_features)
                for i in range(probabilities.shape[1]):
                    class_name = CLASS_MAPPING.get(i, f"Classe_{i}")
                    result_df[f'Confiance_{class_name}'] = probabilities[:, i]
                
                # Ajouter la confiance maximale
                result_df['Confiance_Maximale'] = np.max(probabilities, axis=1)
                result_df['Classe_Plus_Probable'] = [CLASS_MAPPING.get(np.argmax(prob), "Inconnu") for prob in probabilities]
                
            except Exception as proba_error:
                st.warning(f"‚ö†Ô∏è Impossible d'ajouter les probabilit√©s: {proba_error}")
        
        # Statistiques des pr√©dictions
        prediction_stats = pd.DataFrame({
            'Classe': list(CLASS_MAPPING.values()),
            'Nombre': [np.sum(predictions_numeric == i) for i in range(3)],
            'Pourcentage': [f"{(np.sum(predictions_numeric == i) / len(predictions_numeric)) * 100:.2f}%" 
                          for i in range(3)]
        })
        
        return result_df, prediction_stats, None
        
    except Exception as e:
        return None, None, f"Erreur lors du traitement par lot: {str(e)}"

# CHARGEMENT DES MODELES PERSONNALISES ET REENTRAINES
def load_custom_and_retrained_models():
    """Charge les mod√®les personnalis√©s et r√©entra√Æn√©s depuis les fichiers .pkl"""
    custom_models = {}
    
    # Charger tous les fichiers .pkl
    pkl_files = [f for f in os.listdir('.') if f.endswith('.pkl')]
    
    for model_file in pkl_files:
        try:
            model_name = model_file.replace('.pkl', '')
            
            # Ignorer les mod√®les de base (d√©j√† charg√©s s√©par√©ment)
            if model_name in st.session_state.models:
                continue
                
            with open(model_file, 'rb') as f:
                model = pickle.load(f)
            
            # V√©rifier que c'est bien un mod√®le (a une m√©thode predict)
            if hasattr(model, 'predict'):
                custom_models[model_name] = model
                
        except Exception as e:
            st.error(f"‚ùå Erreur lors du chargement de {model_file}: {e}")
    
    return custom_models

# INITIALISATION
if 'models' not in st.session_state:
    st.session_state.models, st.session_state.model_info = load_models_with_dict()

if 'custom_models' not in st.session_state:
    st.session_state.custom_models = load_custom_and_retrained_models()

if 'last_prediction' not in st.session_state:
    st.session_state.last_prediction = None

if 'feedback_given' not in st.session_state:
    st.session_state.feedback_given = False

initialize_features()
metrics_data = load_model_metrics()
comparison_df = load_model_comparison()
dataset = load_dataset()

# INTERFACE UTILISATEUR PRINCIPALE
st.markdown("""
<div class="main-header">
    <h1>üöÄ NASA Exoplanet Prediction</h1>
    <p>Plateforme avanc√©e d'analyse et de pr√©diction d'exoplan√®tes</p>
</div>
""", unsafe_allow_html=True)

# SIDEBAR - DIAGNOSTIC
with st.sidebar:
    st.markdown("## üì¶ Statut des Mod√®les")
    st.write(f"**Mod√®les de base:** {len(st.session_state.models)}")
    st.write(f"**Mod√®les personnalis√©s:** {get_custom_models_count()}")
    st.write(f"**Mod√®les r√©entra√Æn√©s:** {get_retrained_models_count()}")
    
    if st.session_state.models and not comparison_df.empty:
        st.markdown("### üîç Correspondances")
        for model_name in st.session_state.models.keys():
            matching_name = find_matching_model_name(model_name, comparison_df)
            status = "‚úÖ" if matching_name else "‚ùå"
            st.write(f"{status} {model_name} ‚Üí {matching_name if matching_name else 'Aucune'}")

    st.markdown("---")
    st.subheader("üîó Navigation Rapide")
    if st.sidebar.button("üéØ Aller √† la Pr√©diction", use_container_width=True):
        st.rerun()

    if st.session_state.last_prediction is not None:
        if st.sidebar.button("üìä Voir les R√©sultats", use_container_width=True):
            st.rerun()

    if st.sidebar.button("üîÑ Recharger tous les mod√®les", use_container_width=True):
        st.session_state.models, st.session_state.model_info = load_models_with_dict()
        st.session_state.custom_models = load_custom_and_retrained_models()
        st.rerun()

    st.markdown("---")
    st.subheader("üíæ Gestion des Fichiers")
    pkl_files = [f for f in os.listdir('.') if f.endswith('.pkl')]
    for pkl_file in pkl_files:
        st.write(f"‚Ä¢ {pkl_file}")

# ONGLETS PRINCIPAUX
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üéØ Pr√©diction", 
    "üìä R√©sultat & Feedback", 
    "üìà Mod√®les & M√©triques", 
    "‚öôÔ∏è Hyperparameter Tuning", 
    "üîÑ R√©entra√Ænement"
])

# ONGLET 1: PR√âDICTION - CORRIG√â
with tab1:
    st.header("üéØ Pr√©diction d'Exoplan√®tes")
    
    # Combiner tous les mod√®les disponibles
    all_models = {**st.session_state.models, **st.session_state.custom_models}
    
    working_models = [name for name, model in all_models.items() if model is not None and hasattr(model, 'predict')]
    
    if not working_models:
        st.error("‚ùå Aucun mod√®le fonctionnel disponible")
    else:
        selected_model = st.selectbox(
            "Choisir le mod√®le pour la pr√©diction:",
            working_models,
            help="S√©lectionnez un mod√®le parmi les mod√®les disponibles",
            key="model_selector_prediction"
        )
        
        if selected_model:
            model = all_models[selected_model]
            
            # D√©terminer le type de mod√®le
            is_custom_model_flag = is_custom_model(selected_model)
            is_retrained_model_flag = is_retrained_model(selected_model)
            
            # Afficher les m√©triques appropri√©es
            if is_retrained_model_flag:
                # Afficher les m√©triques du mod√®le r√©entra√Æn√©
                retrained_metrics = load_retrained_model_metrics()
                retrained_model_data = None
                for metric in retrained_metrics:
                    if metric['model_name'] == selected_model:
                        retrained_model_data = metric
                        break
                
                if retrained_model_data:
                    display_retrained_metrics_from_json(retrained_model_data, selected_model)
                else:
                    st.info("‚ÑπÔ∏è Mod√®le r√©entra√Æn√© - M√©triques d√©taill√©es non disponibles")
            elif is_custom_model_flag:
                # Afficher les m√©triques du mod√®le personnalis√©
                custom_metrics = load_custom_model_metrics()
                custom_model_data = None
                for metric in custom_metrics:
                    if metric['model_name'] == selected_model:
                        custom_model_data = metric
                        break
                
                if custom_model_data:
                    display_custom_metrics_from_json(custom_model_data, selected_model)
                else:
                    st.info("‚ÑπÔ∏è Mod√®le personnalis√© - M√©triques d√©taill√©es non disponibles")
            else:
                # Afficher les m√©triques du mod√®le de base
                model_info = st.session_state.model_info.get(selected_model, {})
                if not comparison_df.empty:
                    display_all_model_metrics_real_time(selected_model, comparison_df, model_info)
                else:
                    st.warning("‚ÑπÔ∏è Aucune donn√©e de m√©triques disponible")
                    st.info(f"Mod√®le: {selected_model}")
            
            st.markdown("---")
            st.subheader("üìä Caract√©ristiques d'Entr√©e")
            st.write("Veuillez saisir les valeurs pour les caract√©ristiques d'exoplan√®tes:")
            
            features = []
            feature_values_dict = {}
            
            for group_name, group_features in FEATURE_GROUPS.items():
                st.markdown(f'<div class="feature-group"><h4>{group_name}</h4></div>', unsafe_allow_html=True)
                
                cols = st.columns(3)
                for i, (feature_key, feature_name, min_val, max_val, default_val) in enumerate(group_features):
                    with cols[i % 3]:
                        current_value = st.session_state.feature_values.get(feature_key, default_val)
                        new_value = st.number_input(
                            label=feature_name,
                            value=float(current_value),
                            min_value=float(min_val),
                            max_value=float(max_val),
                            step=0.1 if max_val > 10 else 0.01,
                            format="%.6f",
                            key=f"pred_{feature_key}",
                            help=get_feature_description(feature_key)
                        )
                        st.session_state.feature_values[feature_key] = new_value
                        features.append(new_value)
                        feature_values_dict[feature_key] = new_value
            
            if len(features) != 20:
                st.error(f"‚ùå Erreur: Nombre de features incorrect. Attendu: 20, Obtenu: {len(features)}")
            else:
                if st.button("üöÄ Lancer la Pr√©diction", type="primary", use_container_width=True):
                    try:
                        features_array = np.array(features).reshape(1, -1)
                        prediction = model.predict(features_array)
                        
                        probabilities = None
                        if hasattr(model, 'predict_proba'):
                            try:
                                probabilities = model.predict_proba(features_array)
                            except Exception as proba_error:
                                st.warning(f"‚ö†Ô∏è Impossible de r√©cup√©rer les probabilit√©s: {proba_error}")
                        
                        prediction_label = CLASS_MAPPING.get(prediction[0], "Inconnu")
                        
                        st.session_state.last_prediction = {
                            'model': selected_model,
                            'prediction': prediction[0],
                            'prediction_label': prediction_label,
                            'probabilities': probabilities[0] if probabilities is not None else None,
                            'features': features,
                            'feature_values': feature_values_dict,
                            'model_type': 'Retrained' if is_retrained_model_flag else 'Custom' if is_custom_model_flag else 'Base'
                        }
                        st.session_state.feedback_given = False
                        
                        st.success("‚úÖ Pr√©diction effectu√©e avec succ√®s! Redirection vers les r√©sultats...")
                        js = """
                        <script>
                            const tabs = window.parent.document.querySelectorAll('[data-testid="stTab"]');
                            tabs.forEach(tab => {
                                if (tab.textContent.includes('üìä R√©sultat')) {
                                    tab.click();
                                }
                            });
                        </script>
                        """
                        st.components.v1.html(js, height=0)
                        
                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de la pr√©diction: {str(e)}")
            
            # SECTION PR√âDICTION PAR LOT AM√âLIOR√âE
            st.markdown("---")
            st.subheader("üìÅ Pr√©diction par Lot (CSV)")
            
            # Instructions d√©taill√©es
            st.markdown("""
            <div class="batch-prediction-info">
            <h4>üìã Instructions pour la pr√©diction par lot</h4>
            <p><strong>Format requis:</strong> Fichier CSV avec au moins 20 colonnes de caract√©ristiques dans l'ordre suivant :</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Afficher l'ordre des colonnes attendues
            col_order = []
            for group_name, group_features in FEATURE_GROUPS.items():
                for feature_key, feature_name, _, _, _ in group_features:
                    col_order.append(feature_name)
            
            st.info(f"**Ordre des colonnes attendues:** {', '.join(col_order[:5])}...")
            
            with st.expander("üìã Voir l'ordre complet des 20 colonnes"):
                for i, col_name in enumerate(col_order, 1):
                    st.write(f"{i}. {col_name}")
            
            uploaded_file = st.file_uploader("Charger un fichier CSV pour les pr√©dictions par lot", 
                                           type=['csv'], 
                                           key="batch_prediction")
            
            if uploaded_file is not None:
                try:
                    # Charger et valider le fichier
                    batch_df = pd.read_csv(uploaded_file)
                    st.success(f"‚úÖ Fichier charg√©: {batch_df.shape[0]} lignes, {batch_df.shape[1]} colonnes")
                    
                    # Validation du fichier
                    is_valid, validation_message = validate_batch_csv(batch_df)
                    
                    if not is_valid:
                        st.error(f"‚ùå {validation_message}")
                    else:
                        st.success("‚úÖ Fichier valid√© avec succ√®s")
                        
                        # Aper√ßu des donn√©es
                        with st.expander("üëÄ Aper√ßu des donn√©es charg√©es"):
                            st.dataframe(batch_df.head(10))
                            
                            # Statistiques descriptives
                            st.subheader("üìä Statistiques descriptives")
                            st.dataframe(batch_df.describe())
                        
                        # Bouton de pr√©diction par lot
                        if st.button("üéØ Lancer les Pr√©dictions par Lot", 
                                   use_container_width=True, 
                                   key="batch_predict",
                                   type="primary"):
                            
                            with st.spinner("Traitement des pr√©dictions par lot en cours..."):
                                # Traitement des pr√©dictions
                                result_df, prediction_stats, error = process_batch_prediction(model, batch_df, selected_model)
                                
                                if error:
                                    st.error(f"‚ùå {error}")
                                else:
                                    st.success(f"‚úÖ Pr√©dictions termin√©es! {len(result_df)} lignes trait√©es")
                                    
                                    # Afficher les statistiques des pr√©dictions
                                    st.subheader("üìà Statistiques des Pr√©dictions")
                                    col1, col2, col3 = st.columns(3)
                                    
                                    with col1:
                                        st.metric("Total des pr√©dictions", len(result_df))
                                    with col2:
                                        exoplanets_count = len(result_df[result_df['Prediction_Classe'] == 'Exoplan√®te'])
                                        st.metric("Exoplan√®tes d√©tect√©es", exoplanets_count)
                                    with col3:
                                        candidates_count = len(result_df[result_df['Prediction_Classe'] == 'Candidat'])
                                        st.metric("Candidats identifi√©s", candidates_count)
                                    
                                    # Graphique de distribution des classes
                                    fig, ax = plt.subplots(figsize=(10, 6))
                                    class_counts = result_df['Prediction_Classe'].value_counts()
                                    colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']
                                    bars = ax.bar(class_counts.index, class_counts.values, color=colors)
                                    ax.set_title('Distribution des Classes Pr√©dites')
                                    ax.set_ylabel('Nombre d\'occurrences')
                                    
                                    # Ajouter les valeurs sur les barres
                                    for bar in bars:
                                        height = bar.get_height()
                                        ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                                                f'{int(height)}', ha='center', va='bottom')
                                    
                                    plt.xticks(rotation=45)
                                    plt.tight_layout()
                                    st.pyplot(fig)
                                    
                                    # Aper√ßu des r√©sultats
                                    st.subheader("üëÄ Aper√ßu des R√©sultats")
                                    st.dataframe(result_df.head(15))
                                    
                                    # T√©l√©chargement des r√©sultats
                                    st.subheader("üì• T√©l√©chargement des R√©sultats")
                                    csv = result_df.to_csv(index=False)
                                    
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        st.download_button(
                                            label="üíæ T√©l√©charger CSV Complet",
                                            data=csv,
                                            file_name=f"predictions_complete_{selected_model}.csv",
                                            mime="text/csv",
                                            use_container_width=True
                                        )
                                    with col2:
                                        # Version simplifi√©e pour analyse rapide
                                        simple_cols = ['Prediction_Classe', 'Confiance_Maximale', 'Classe_Plus_Probable']
                                        available_cols = [col for col in simple_cols if col in result_df.columns]
                                        simple_df = result_df[available_cols]
                                        simple_csv = simple_df.to_csv(index=False)
                                        
                                        st.download_button(
                                            label="üìÑ T√©l√©charger R√©sum√©",
                                            data=simple_csv,
                                            file_name=f"predictions_summary_{selected_model}.csv",
                                            mime="text/csv",
                                            use_container_width=True
                                        )
                                    
                                    # Section d'analyse avanc√©e
                                    if 'Confiance_Maximale' in result_df.columns:
                                        st.markdown("---")
                                        st.subheader("üîç Analyse de Confiance")
                                        
                                        col1, col2 = st.columns(2)
                                        
                                        with col1:
                                            # Distribution de la confiance
                                            fig, ax = plt.subplots(figsize=(10, 6))
                                            ax.hist(result_df['Confiance_Maximale'], bins=20, alpha=0.7, color='#667eea')
                                            ax.set_xlabel('Confiance Maximale')
                                            ax.set_ylabel('Fr√©quence')
                                            ax.set_title('Distribution de la Confiance des Pr√©dictions')
                                            st.pyplot(fig)
                                        
                                        with col2:
                                            # Seuil de confiance
                                            confidence_threshold = st.slider(
                                                "Seuil de confiance pour filtrer les r√©sultats:",
                                                0.5, 1.0, 0.8, 0.05
                                            )
                                            
                                            high_confidence = result_df[result_df['Confiance_Maximale'] >= confidence_threshold]
                                            st.metric(
                                                f"Pr√©dictions avec confiance ‚â• {confidence_threshold}",
                                                f"{len(high_confidence)} ({len(high_confidence)/len(result_df)*100:.1f}%)"
                                            )
                                            
                                            if len(high_confidence) > 0:
                                                st.dataframe(high_confidence[['Prediction_Classe', 'Confiance_Maximale']].head(10))
                            
                except Exception as e:
                    st.error(f"‚ùå Erreur lors du traitement du fichier CSV: {str(e)}")

# ONGLET 2: R√âSULTAT & FEEDBACK
with tab2:
    st.header("üìä R√©sultat de la Pr√©diction")
    
    if st.session_state.last_prediction is None:
        st.info("‚ÑπÔ∏è Aucune pr√©diction r√©cente. Veuillez d'abord faire une pr√©diction dans l'onglet 'Pr√©diction'.")
    else:
        prediction_data = st.session_state.last_prediction
        
        st.markdown(f"""
        <div class="prediction-result">
            <h2>üéØ R√©sultat de la Pr√©diction</h2>
            <h3>Classe Pr√©dite: {prediction_data['prediction_label']}</h3>
            <p><strong>Mod√®le utilis√©:</strong> {prediction_data['model']}</p>
            <p><strong>Type de mod√®le:</strong> {prediction_data.get('model_type', 'Inconnu')}</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.subheader("üìã Valeurs des Caract√©ristiques Utilis√©es")
        
        feature_display_data = []
        for group_name, group_features in FEATURE_GROUPS.items():
            for feature_key, feature_name, min_val, max_val, default_val in group_features:
                if feature_key in prediction_data['feature_values']:
                    feature_display_data.append({
                        'Groupe': group_name,
                        'Caract√©ristique': feature_name,
                        'Valeur': prediction_data['feature_values'][feature_key]
                    })
        
        feature_df = pd.DataFrame(feature_display_data)
        st.dataframe(feature_df, use_container_width=True)
        
        if prediction_data['probabilities'] is not None:
            st.subheader("üìä Probabilit√©s par Classe")
            
            prob_data = []
            for i, prob in enumerate(prediction_data['probabilities']):
                class_name = CLASS_MAPPING.get(i, f"Classe {i}")
                prob_data.append({'Classe': class_name, 'Probabilit√©': prob})
            
            prob_df = pd.DataFrame(prob_data)
            
            fig, ax = plt.subplots(figsize=(10, 6))
            colors = ['#ff6b6b', '#4ecdc4', '#45b7d1']
            bars = ax.bar(prob_df['Classe'], prob_df['Probabilit√©'], color=colors[:len(prob_df)])
            ax.set_ylabel('Probabilit√©')
            ax.set_title('Probabilit√©s de Pr√©diction par Classe')
            ax.set_ylim(0, 1)
            
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{height:.3f}', ha='center', va='bottom', fontweight='bold')
            
            plt.xticks(rotation=45)
            plt.tight_layout()
            st.pyplot(fig)
            
            st.dataframe(prob_df)
        else:
            st.warning("‚ö†Ô∏è Les probabilit√©s ne sont pas disponibles pour ce mod√®le")
        
        if not st.session_state.feedback_given:
            st.markdown("---")
            st.subheader("üí¨ Feedback sur la Pr√©diction")
            
            st.write("Cette pr√©diction vous semble-t-elle correcte?")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if st.button("üëç Correct", use_container_width=True, key="feedback_correct"):
                    st.session_state.feedback_given = True
                    st.success("‚úÖ Merci pour votre feedback!")
            with col2:
                if st.button("üëé Incorrect", use_container_width=True, key="feedback_incorrect"):
                    st.session_state.feedback_given = True
                    st.warning("‚ö†Ô∏è Merci pour votre feedback, nous am√©liorons constamment nos mod√®les.")
            with col3:
                if st.button("ü§î Incertain", use_container_width=True, key="feedback_uncertain"):
                    st.session_state.feedback_given = True
                    st.info("‚ÑπÔ∏è Merci pour votre retour!")
        else:
            st.success("‚úÖ Feedback d√©j√† donn√© pour cette pr√©diction.")

# ONGLET 3: MOD√àLES & M√âTRIQUES
with tab3:
    st.header("üìà Analyse des Mod√®les")
    
    if comparison_df.empty:
        st.error("‚ùå Donn√©es de comparaison non disponibles")
    else:
        display_model_comparison(comparison_df)
        
        st.markdown("---")
        st.subheader("üîç Analyse D√©taill√©e par Mod√®le")
        
        selected_model_detail = st.selectbox(
            "Choisir un mod√®le pour voir les d√©tails:",
            comparison_df['model_name'].tolist(),
            key="model_detail_select"
        )
        
        if selected_model_detail:
            display_model_metrics(selected_model_detail, comparison_df)
        
        st.markdown("---")
        st.subheader("üîß Diagnostic des Mod√®les Charg√©s")
        
        if st.session_state.models:
            st.write(f"**Mod√®les charg√©s avec succ√®s:** {len([m for m in st.session_state.model_info.values() if m['has_model']])}")
            st.write(f"**Mod√®les avec probabilit√©s:** {len([m for m in st.session_state.model_info.values() if m.get('has_predict_proba', False)])}")
            
            with st.expander("üìã D√©tails du diagnostic des mod√®les"):
                for model_name, info in st.session_state.model_info.items():
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        status = "‚úÖ" if info['has_model'] else "‚ùå"
                        proba_status = "‚úÖ" if info.get('has_predict_proba', False) else "‚ùå"
                        st.write(f"{status} **{model_name}** - {info['model_type']} - Probabilit√©s: {proba_status}")
                    with col2:
                        if info.get('test_error'):
                            st.error("Erreur test")
                        elif info.get('test_success'):
                            st.success("Test OK")

# ONGLET 4: HYPERPARAMETER TUNING - CORRIG√â
with tab4:
    st.header("‚öôÔ∏è Personnalisation des Hyperparam√®tres")
    
    st.markdown("""
    <div class="metrics-container">
    <h3>üéØ Cr√©ation de Mod√®les Personnalis√©s</h3>
    <p>Configurez vos propres hyperparam√®tres pour cr√©er un mod√®le personnalis√©. Le mod√®le sera automatiquement 
    entra√Æn√© sur le dataset Kepler et √©valu√©. Maximum 10 mod√®les personnalis√©s.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # S√©lection du type de mod√®le
    model_type = st.selectbox(
        "Type de mod√®le √† personnaliser:",
        ["RandomForest", "XGBoost", "SVM"],
        key="model_type_select"
    )
    
    st.subheader("üéõÔ∏è Configuration des Hyperparam√®tres")
    
    # Configuration selon le type de mod√®le
    if model_type == "RandomForest":
        col1, col2 = st.columns(2)
        with col1:
            n_estimators = st.slider("n_estimators", 50, 500, 100, 50, 
                                   help="Nombre d'arbres dans la for√™t")
            max_depth = st.slider("max_depth", 5, 50, 20, 
                                help="Profondeur maximale des arbres")
        with col2:
            min_samples_split = st.slider("min_samples_split", 2, 10, 2, 
                                        help="Nombre minimum d'√©chantillons pour diviser un n≈ìud")
            min_samples_leaf = st.slider("min_samples_leaf", 1, 5, 1, 
                                       help="Nombre minimum d'√©chantillons dans une feuille")
        
        max_features = st.selectbox("max_features", ["sqrt", "log2", "auto"], 
                                  help="Nombre de features √† consid√©rer pour la meilleure division")
        
        hyperparams = {
            'n_estimators': n_estimators,
            'max_depth': max_depth,
            'min_samples_split': min_samples_split,
            'min_samples_leaf': min_samples_leaf,
            'max_features': max_features
        }
    
    elif model_type == "XGBoost":
        col1, col2 = st.columns(2)
        with col1:
            n_estimators = st.slider("n_estimators", 50, 500, 100, 50,
                                   help="Nombre d'arbres boosting")
            max_depth = st.slider("max_depth", 3, 15, 7,
                                help="Profondeur maximale des arbres")
        with col2:
            learning_rate = st.slider("learning_rate", 0.01, 0.5, 0.1, 0.01,
                                    help="Taux d'apprentissage pour le boosting")
            subsample = st.slider("subsample", 0.5, 1.0, 0.8, 0.1,
                                help="Fraction d'√©chantillons utilis√©s pour l'entra√Ænement")
        
        colsample_bytree = st.slider("colsample_bytree", 0.5, 1.0, 0.8, 0.1,
                                   help="Fraction de features utilis√©es pour chaque arbre")
        
        hyperparams = {
            'n_estimators': n_estimators,
            'max_depth': max_depth,
            'learning_rate': learning_rate,
            'subsample': subsample,
            'colsample_bytree': colsample_bytree
        }
    
    elif model_type == "SVM":
        col1, col2 = st.columns(2)
        with col1:
            C = st.slider("C", 0.1, 10.0, 1.0, 0.1,
                        help="Param√®tre de r√©gularisation")
            kernel = st.selectbox("kernel", ["rbf", "linear", "poly", "sigmoid"],
                               help="Type de noyau √† utiliser")
        with col2:
            gamma = st.selectbox("gamma", ["scale", "auto"],
                               help="Coefficient du noyau")
            degree = st.slider("degree", 2, 5, 3,
                             help="Degr√© du polyn√¥me (si kernel='poly')") if kernel == "poly" else 3
        
        hyperparams = {
            'C': C,
            'kernel': kernel,
            'gamma': gamma,
            'degree': degree
        }
    
    # Nom du mod√®le personnalis√©
    st.subheader("üè∑Ô∏è Configuration du Mod√®le")
    
    # G√©n√©rer un nom unique pour le mod√®le
    existing_custom_names = [k for k in st.session_state.custom_models.keys() if is_custom_model(k)]
    base_name = f"Custom_{model_type}"
    counter = 1
    while f"{base_name}_{counter}" in existing_custom_names:
        counter += 1
    default_name = f"{base_name}_{counter}"
    
    custom_model_name = st.text_input("Nom du mod√®le personnalis√©:", 
                                    default_name, 
                                    key="custom_model_name",
                                    help="Donnez un nom unique √† votre mod√®le personnalis√©")
    
    # Informations sur les hyperparam√®tres
    st.markdown("---")
    st.subheader("üìö Explications des Hyperparam√®tres")
    
    if model_type == "RandomForest":
        st.markdown("""
        <div class="hyperparam-info">
        <strong>Random Forest - Explications:</strong><br>
        ‚Ä¢ <strong>n_estimators</strong>: Nombre d'arbres dans la for√™t. Plus d'arbres am√©liore la performance mais augmente le temps de calcul.<br>
        ‚Ä¢ <strong>max_depth</strong>: Profondeur maximale des arbres. √âvite le sur-apprentissage en limitant la complexit√©.<br>
        ‚Ä¢ <strong>min_samples_split</strong>: Nombre minimum d'√©chantillons requis pour diviser un n≈ìud interne.<br>
        ‚Ä¢ <strong>min_samples_leaf</strong>: Nombre minimum d'√©chantillons requis dans un n≈ìud feuille.<br>
        ‚Ä¢ <strong>max_features</strong>: Nombre de features √† consid√©rer pour chercher la meilleure division.
        </div>
        """, unsafe_allow_html=True)
    
    elif model_type == "XGBoost":
        st.markdown("""
        <div class="hyperparam-info">
        <strong>XGBoost - Explications:</strong><br>
        ‚Ä¢ <strong>n_estimators</strong>: Nombre d'arbres de boosting. Contr√¥le le nombre de rounds de boosting.<br>
        ‚Ä¢ <strong>max_depth</strong>: Profondeur maximale des arbres. Une valeur plus √©lev√©e permet des mod√®les plus complexes.<br>
        ‚Ä¢ <strong>learning_rate</strong>: Taux d'apprentissage. R√©duit l'impact de chaque arbre pour √©viter le sur-apprentissage.<br>
        ‚Ä¢ <strong>subsample</strong>: Fraction d'√©chantillons utilis√©s pour l'entra√Ænement. Pr√©vention du sur-apprentissage.<br>
        ‚Ä¢ <strong>colsample_bytree</strong>: Fraction de features utilis√©es pour construire chaque arbre.
        </div>
        """, unsafe_allow_html=True)
    
    elif model_type == "SVM":
        st.markdown("""
        <div class="hyperparam-info">
        <strong>SVM - Explications:</strong><br>
        ‚Ä¢ <strong>C</strong>: Param√®tre de r√©gularisation. Contr√¥le le trade-off entre marge d'erreur et complexit√© du mod√®le.<br>
        ‚Ä¢ <strong>kernel</strong>: Fonction noyau utilis√©e pour transformer les donn√©es (lin√©aire, RBF, polynomial, sigmo√Øde).<br>
        ‚Ä¢ <strong>gamma</strong>: Coefficient du noyau. D√©finit l'influence d'un seul exemple d'entra√Ænement.<br>
        ‚Ä¢ <strong>degree</strong>: Degr√© de la fonction noyau polynomial (si kernel='poly').
        </div>
        """, unsafe_allow_html=True)
    
    # Boutons de cr√©ation et gestion
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üöÄ Cr√©er et Entra√Æner le Mod√®le", type="primary", use_container_width=True, key="create_custom_model"):
            if not custom_model_name.strip():
                st.error("‚ùå Veuillez donner un nom √† votre mod√®le")
            elif custom_model_name in st.session_state.models or custom_model_name in st.session_state.custom_models:
                st.error("‚ùå Ce nom de mod√®le existe d√©j√†")
            else:
                with st.spinner("Cr√©ation et entra√Ænement du mod√®le personnalis√© en cours..."):
                    # V√©rifier et g√©rer la limite de 10 mod√®les
                    custom_models_count = get_custom_models_count()
                    if custom_models_count >= 10:
                        # Trouver le mod√®le custom le plus ancien
                        oldest_custom = None
                        for model_name in st.session_state.custom_models.keys():
                            if is_custom_model(model_name):
                                oldest_custom = model_name
                                break
                        
                        if oldest_custom:
                            # Supprimer le mod√®le le plus ancien
                            delete_custom_model(oldest_custom)
                            st.info(f"üóëÔ∏è Mod√®le '{oldest_custom}' supprim√© (limite de 10 mod√®les atteinte)")
                    
                    # Cr√©er le mod√®le
                    custom_model = create_custom_model(model_type, hyperparams)
                    
                    if custom_model is not None:
                        # Charger les donn√©es d'entra√Ænement r√©elles
                        X_train, X_test, y_train, y_test = load_training_data()
                        
                        if X_train is not None:
                            # Entra√Æner et √©valuer le mod√®le
                            metrics = train_and_evaluate_model(custom_model, X_train, X_test, y_train, y_test)
                            
                            if metrics:
                                # Sauvegarder les m√©triques dans JSON
                                save_success = save_custom_model_metrics(custom_model_name, metrics, hyperparams)
                                
                                # Stocker directement l'objet mod√®le
                                st.session_state.custom_models[custom_model_name] = custom_model
                                
                                # Sauvegarde du mod√®le
                                try:
                                    with open(f"{custom_model_name}.pkl", 'wb') as f:
                                        pickle.dump(custom_model, f)
                                    
                                    st.success(f"‚úÖ Mod√®le personnalis√© '{custom_model_name}' cr√©√© et entra√Æn√© avec succ√®s!")
                                    st.metric("Accuracy obtenue", f"{metrics['accuracy']:.4f}")
                                    
                                    # Afficher les m√©triques imm√©diatement
                                    display_custom_metrics_from_json({
                                        'model_name': custom_model_name,
                                        **metrics,
                                        'hyperparameters': hyperparams
                                    }, custom_model_name)
                                    
                                    # Recharger la page pour mettre √† jour la liste des mod√®les
                                    st.rerun()
                                    
                                except Exception as e:
                                    st.error(f"‚ùå Erreur lors de la sauvegarde: {e}")
                        else:
                            st.error("‚ùå Impossible de charger les donn√©es d'entra√Ænement")
    
    with col2:
        custom_models_count = get_custom_models_count()
        if custom_models_count > 0:
            if st.button("üóëÔ∏è Supprimer Tous les Mod√®les Personnalis√©s", type="secondary", use_container_width=True, key="delete_all_custom"):
                delete_all_custom_models()
                st.rerun()
    
    # Affichage des mod√®les personnalis√©s existants
    custom_models_list = [k for k in st.session_state.custom_models.keys() if is_custom_model(k)]
    if custom_models_list:
        st.markdown("---")
        st.subheader("üìã Mod√®les Personnalis√©s Existants")
        
        for model_name in custom_models_list[:10]:  # Limite √† 10
            col1, col2, col3 = st.columns([3, 2, 1])
            with col1:
                st.write(f"**{model_name}**")
                # Charger les m√©triques depuis le fichier JSON pour affichage
                custom_metrics = load_custom_model_metrics()
                model_metrics = None
                for metric in custom_metrics:
                    if metric['model_name'] == model_name:
                        model_metrics = metric
                        break
                
                if model_metrics:
                    st.write(f"Accuracy: {model_metrics.get('accuracy', 0):.4f}")
            
            with col2:
                # Essayer de r√©cup√©rer le timestamp depuis les m√©triques
                if model_metrics and 'creation_time' in model_metrics:
                    try:
                        creation_time = pd.to_datetime(model_metrics['creation_time'])
                        st.write(f"Cr√©√© le: {creation_time.strftime('%Y-%m-%d %H:%M')}")
                    except:
                        st.write("Date inconnue")
                else:
                    st.write("Date inconnue")
            
            with col3:
                if st.button("üóëÔ∏è", key=f"delete_{model_name}"):
                    success = delete_custom_model(model_name)
                    if success:
                        st.success(f"‚úÖ Mod√®le '{model_name}' supprim√©")
                        st.rerun()

# ONGLET 5: R√âENTRA√éNEMENT AVEC PR√âPROCESSING AVANC√â
with tab5:
    # Initialize session_state variables
    if 'processed_data' not in st.session_state:
        st.session_state.processed_data = None
    if 'merge_stats' not in st.session_state:
        st.session_state.merge_stats = None
    if 'scaled_data' not in st.session_state:
        st.session_state.scaled_data = None
    
    st.header("üîÑ R√©entra√Ænement des Mod√®les avec Nouvelles Donn√©es")
    
    st.markdown("""
    <div class="retrain-info">
    <h3>üéØ R√©entra√Ænement Avanc√© avec Pr√©processing</h3>
    <p>Chargez de nouvelles donn√©es pour r√©entra√Æner un mod√®le existant. Les nouvelles donn√©es subiront le m√™me 
    pr√©processing avanc√© que le dataset original (nettoyage, imputation, encodage de koi_disposition, feature engineering) 
    avant d'√™tre fusionn√©es avec le dataset Kepler original.</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Charger le dataset de base pour le r√©entra√Ænement
    base_dataset = load_base_dataset_for_retraining()
    
    if base_dataset is None:
        st.error("‚ùå Impossible de charger le dataset de base pour le r√©entra√Ænement")
        st.info("‚ÑπÔ∏è Assurez-vous que le fichier 'kepler_before_scaling_selected.csv' est pr√©sent dans le r√©pertoire")
    else:
        st.success(f"‚úÖ Dataset de base charg√©: {base_dataset.shape[0]} √©chantillons, {base_dataset.shape[1]} caract√©ristiques")
        st.info(f"üìã Features attendues: {list(base_dataset.columns)}")
    
    # Section de chargement des donn√©es
    st.subheader("üìÅ Chargement des Nouvelles Donn√©es")
    
    uploaded_file = st.file_uploader(
        "Charger un nouveau dataset CSV", 
        type=['csv'], 
        key="retrain_upload",
        help="Le dataset subira un pr√©processing avanc√© avant la fusion (incluant l'encodage de koi_disposition)"
    )
    
    if uploaded_file is not None and base_dataset is not None:
        try:
            new_data = pd.read_csv(uploaded_file)
            st.success(f"‚úÖ Nouveau dataset charg√©: {new_data.shape[0]} lignes, {new_data.shape[1]} colonnes")
            
            # Validation du dataset
            if validate_dataset_for_retraining(new_data, base_dataset):
                st.success("‚úÖ Structure du dataset valid√©e")
                
                # Aper√ßu des donn√©es
                with st.expander("üëÄ Aper√ßu du nouveau dataset"):
                    st.dataframe(new_data.head())
                    st.write(f"**Dimensions:** {new_data.shape[0]} lignes √ó {new_data.shape[1]} colonnes")
                    
                    # V√©rification des features n√©cessaires pour le feature engineering
                    required_engineering_features = ['koi_period', 'koi_prad', 'koi_teq', 'koi_duration']
                    available_features = [f for f in required_engineering_features if f in new_data.columns]
                    missing_features = set(required_engineering_features) - set(available_features)
                    
                    if missing_features:
                        st.warning(f"‚ö†Ô∏è Features manquantes pour l'ing√©nierie: {missing_features}")
                    else:
                        st.success("‚úÖ Toutes les features n√©cessaires pour l'ing√©nierie sont pr√©sentes")
                    
                    # V√©rification de koi_disposition
                    if 'koi_disposition' in new_data.columns:
                        st.success("‚úÖ Colonne koi_disposition trouv√©e - elle sera encod√©e et supprim√©e")
                        # Afficher la distribution des valeurs
                        disposition_distribution = new_data['koi_disposition'].value_counts()
                        st.write("üìä Distribution de koi_disposition:")
                        for value, count in disposition_distribution.items():
                            st.write(f"  - {value}: {count} √©chantillons")
                    else:
                        st.warning("‚ö†Ô∏è Colonne koi_disposition non trouv√©e - v√©rifiez que la variable cible est pr√©sente")
                    
                    # Distribution des classes dans le nouveau dataset
                    if new_data.shape[1] > 0 and 'koi_disposition' in new_data.columns:
                        target_col = new_data['koi_disposition']
                        class_distribution = target_col.value_counts()
                        st.write("üìä Distribution des classes dans le nouveau dataset:")
                        for class_val, count in class_distribution.items():
                            st.write(f"  - {class_val}: {count} √©chantillons")
                
                # Section de pr√©traitement avanc√©
                st.subheader("üîÑ Pr√©traitement Avanc√© du Dataset Upload√©")
                
                # Options de pr√©traitement
                col1, col2 = st.columns(2)
                with col1:
                    remove_duplicates = st.checkbox("Supprimer les doublons", value=True, 
                                                   help="Supprime les lignes en double")
                with col2:
                    remove_na = st.checkbox("Supprimer les valeurs manquantes r√©siduelles", value=True,
                                           help="Supprime les lignes avec des valeurs manquantes apr√®s imputation")
                
                # Bouton de pr√©traitement avanc√©
                if st.button("üîç Appliquer le Pr√©processing Avanc√©", key="preprocess_uploaded"):
                    with st.spinner("Pr√©traitement avanc√© en cours..."):
                        # Pr√©traiter le dataset upload√© avec le pr√©processing avanc√©
                        processed_data, preprocess_stats = preprocess_uploaded_dataset(new_data, base_dataset)
                        
                        if processed_data is not None:
                            # Stocker dans session_state pour utilisation ult√©rieure
                            st.session_state.processed_data = processed_data
                            st.session_state.preprocess_stats = preprocess_stats
                            st.success("‚úÖ Pr√©traitement avanc√© termin√© avec succ√®s!")
                            
                            # Afficher les statistiques de pr√©traitement
                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("√âchantillons originaux", preprocess_stats['original_samples'])
                            with col2:
                                st.metric("√âchantillons finaux", preprocess_stats['final_samples'])
                            with col3:
                                st.metric("Features originelles", preprocess_stats['original_features'])
                            with col4:
                                st.metric("Features finales", preprocess_stats['final_features'])
                            
                            # V√©rifier si koi_disposition_encoded est pr√©sente
                            if 'koi_disposition_encoded' in processed_data.columns:
                                st.success("‚úÖ koi_disposition encod√©e avec succ√®s en koi_disposition_encoded")
                                
                                # Afficher la distribution des classes encod√©es
                                encoded_distribution = processed_data['koi_disposition_encoded'].value_counts().sort_index()
                                st.write("üìä Distribution des classes apr√®s encodage:")
                                for class_val, count in encoded_distribution.items():
                                    class_name = CLASS_MAPPING.get(int(class_val), f"Classe {class_val}")
                                    st.write(f"  - {class_name} ({class_val}): {count} √©chantillons")
                            
                # Section de fusion avec le dataset original
                st.subheader("üîÑ Fusion avec le Dataset Original")
                
                st.info(f"üìä Dataset original Kepler: {base_dataset.shape[0]} √©chantillons")
                
                # Simulation de la fusion
                if st.button("üîç Simuler la fusion", key="simulate_merge"):
                    if 'processed_data' not in st.session_state:
                        st.error("‚ùå Veuillez d'abord appliquer le pr√©processing avanc√©")
                    else:
                        with st.spinner("Simulation de la fusion en cours..."):
                            try:
                                # Fusionner les datasets
                                merged_data, stats = merge_and_clean_datasets(
                                    base_dataset, st.session_state.processed_data, remove_duplicates, remove_na
                                )
                                
                                if merged_data is not None:
                                    st.session_state.merged_data = merged_data
                                    st.session_state.merge_stats = stats
                                    st.success(f"‚úÖ Fusion simul√©e r√©ussie!")
                                    
                                    # Afficher les statistiques de fusion
                                    col1, col2, col3, col4 = st.columns(4)
                                    
                                    with col1:
                                        st.metric("Taille originale", f"{stats['original_size']}")
                                    with col2:
                                        st.metric("Doublons supprim√©s", f"{stats['duplicates_removed']}")
                                    with col3:
                                        st.metric("NA supprim√©s", f"{stats['na_removed']}")
                                    with col4:
                                        st.metric("Taille finale", f"{stats['final_size']}")
                                    
                                    # Distribution des classes apr√®s fusion
                                    target_col = merged_data.iloc[:, -1]
                                    class_distribution = target_col.value_counts().sort_index()
                                    st.write("üìä Distribution des classes apr√®s fusion:")
                                    for class_val, count in class_distribution.items():
                                        class_name = CLASS_MAPPING.get(int(class_val), f"Classe {class_val}")
                                        st.write(f"  - {class_name}: {count} √©chantillons")
                                    
                                    # Appliquer le scaling
                                    st.subheader("‚öñÔ∏è Application du Scaling")
                                    scaled_data = apply_robust_scaling(merged_data)
                                    
                                    if scaled_data is not None:
                                        st.session_state.scaled_data = scaled_data
                                        st.success("‚úÖ Scaling appliqu√© avec succ√®s!")
                                        
                                else:
                                    st.error("‚ùå Erreur lors de la simulation de fusion")
                                
                            except Exception as e:
                                st.error(f"‚ùå Erreur lors de la simulation: {str(e)}")
                
                # Afficher les r√©sultats de simulation s'ils existent
                if 'merge_stats' in st.session_state:
                    st.info("üìã R√©sultats de simulation disponibles")
                
                # Section de s√©lection du mod√®le
                st.subheader("üéØ S√©lection du Mod√®le √† R√©entra√Æner")
                
                # Combiner tous les mod√®les disponibles (base + personnalis√©s + r√©entra√Æn√©s)
                all_available_models = {**st.session_state.models, **st.session_state.custom_models}
                working_models = {name: model for name, model in all_available_models.items() 
                                if model is not None and hasattr(model, 'predict') and hasattr(model, 'fit')}
                
                if not working_models:
                    st.error("‚ùå Aucun mod√®le fonctionnel disponible pour le r√©entra√Ænement")
                else:
                    selected_retrain_model = st.selectbox(
                        "Mod√®le √† r√©entra√Æner:",
                        list(working_models.keys()),
                        key="retrain_model_select",
                        help="S√©lectionnez le mod√®le que vous souhaitez r√©entra√Æner avec les nouvelles donn√©es"
                    )
                    
                    if selected_retrain_model:
                        # Afficher les informations du mod√®le s√©lectionn√©
                        model_obj = working_models[selected_retrain_model]
                        st.info(f"**Type de mod√®le:** {type(model_obj).__name__}")
                        
                        # G√©n√©rer un nom pour le mod√®le r√©entra√Æn√©
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        retrained_model_name = f"Retrained_{selected_retrain_model}_{timestamp}"
                        
                        st.write(f"**Nouveau nom du mod√®le:** {retrained_model_name}")
                        
                        # Bouton de r√©entra√Ænement complet
                        if st.button("üîÑ Lancer le R√©entra√Ænement Complet", type="primary", use_container_width=True, key="retrain_button"):
                            with st.spinner("R√©entra√Ænement en cours... Cela peut prendre quelques minutes."):
                                try:
                                    # V√©rifier que les donn√©es n√©cessaires sont dans session_state
                                    if ('processed_data' not in st.session_state or 
                                        'merge_stats' not in st.session_state or 
                                        'scaled_data' not in st.session_state):
                                        st.error("‚ùå Veuillez d'abord simuler la fusion compl√®te")
                                        st.stop()

                                    processed_data = st.session_state.processed_data
                                    merged_data = st.session_state.merged_data
                                    scaled_data = st.session_state.scaled_data
                                    
                                    # Cloner le mod√®le original (important pour ne pas le modifier)
                                    original_model = working_models[selected_retrain_model]
                                    
                                    # Pour scikit-learn models, nous pouvons utiliser clone
                                    from sklearn.base import clone
                                    try:
                                        model_to_retrain = clone(original_model)
                                    except:
                                        # Fallback: utiliser le m√™me type de mod√®le avec m√™mes param√®tres
                                        st.warning("‚ö†Ô∏è Impossible de cloner le mod√®le, utilisation du mod√®le original")
                                        model_to_retrain = original_model
                                    
                                    # R√©entra√Æner le mod√®le sur les donn√©es fusionn√©es et scaled
                                    retrained_model, metrics, error = retrain_model_on_merged_data(
                                        model_to_retrain, scaled_data
                                    )
                                    
                                    if error:
                                        st.error(f"‚ùå {error}")
                                    else:
                                        st.success("‚úÖ R√©entra√Ænement termin√© avec succ√®s!")
                                        
                                        # Sauvegarder le mod√®le r√©entra√Æn√©
                                        with open(f"{retrained_model_name}.pkl", 'wb') as f:
                                            pickle.dump(retrained_model, f)
                                        
                                        # Ajouter au session_state
                                        st.session_state.custom_models[retrained_model_name] = retrained_model
                                        
                                        # Combiner les statistiques
                                        combined_stats = {
                                            **st.session_state.merge_stats,
                                            'preprocessing_stats': st.session_state.preprocess_stats,
                                            'base_dataset_size': len(base_dataset),
                                            'uploaded_dataset_size': len(new_data),
                                            'final_dataset_size': len(scaled_data)
                                        }
                                        
                                        # Sauvegarder les m√©triques
                                        save_success = save_retrained_model_metrics(
                                            retrained_model_name, metrics, selected_retrain_model, combined_stats
                                        )
                                        
                                        if save_success:
                                            st.success(f"üíæ Mod√®le r√©entra√Æn√© sauvegard√© sous '{retrained_model_name}.pkl'")
                                            
                                            # Afficher les m√©triques compl√®tes
                                            display_retrained_metrics_from_json({
                                                'model_name': retrained_model_name,
                                                'original_model': selected_retrain_model,
                                                **metrics,
                                                'dataset_stats': combined_stats,
                                                'retrain_time': pd.Timestamp.now().isoformat()
                                            }, retrained_model_name)
                                            
                                            st.balloons()
                                            st.rerun()
                                            st.markdown("---")
                                            st.success("üéâ R√©entra√Ænement termin√© avec succ√®s! Le mod√®le est maintenant disponible dans la liste des mod√®les.")
                                            st.info(f"üîç Le mod√®le '{retrained_model_name}' est maintenant s√©lectionnable dans l'onglet 'Pr√©diction'")
                                    
                                except Exception as e:
                                    st.error(f"‚ùå Erreur lors du r√©entra√Ænement: {str(e)}")                                    # 4. Cloner le mod√®le original (important pour ne pas le modifier)
                                    original_model = working_models[selected_retrain_model]
                                    
                                    # Pour scikit-learn models, nous pouvons utiliser clone
                                    from sklearn.base import clone
                                    try:
                                        model_to_retrain = clone(original_model)
                                    except:
                                        # Fallback: utiliser le m√™me type de mod√®le avec m√™mes param√®tres
                                        st.warning("‚ö†Ô∏è Impossible de cloner le mod√®le, utilisation du mod√®le original")
                                        model_to_retrain = original_model
                                    
                                    # 5. R√©entra√Æner le mod√®le sur les donn√©es fusionn√©es et scaled
                                    retrained_model, metrics, error = retrain_model_on_merged_data(
                                        model_to_retrain, scaled_data
                                    )
                                    
                                    if error:
                                        st.error(f"‚ùå {error}")
                                    else:
                                        st.success("‚úÖ R√©entra√Ænement termin√© avec succ√®s!")
                                        
                                        # Afficher les m√©triques
                                        st.subheader("üìä M√©triques du Mod√®le R√©entra√Æn√©")
                                        col1, col2, col3, col4 = st.columns(4)
                                        
                                        with col1:
                                            st.metric("Accuracy", f"{metrics['accuracy']:.4f}")
                                        with col2:
                                            st.metric("Precision", f"{metrics['precision']:.4f}")
                                        with col3:
                                            st.metric("Recall", f"{metrics['recall']:.4f}")
                                        with col4:
                                            st.metric("F1 Score", f"{metrics['f1_weighted']:.4f}")
                                        
                                        # Sauvegarder le mod√®le r√©entra√Æn√©
                                        try:
                                            with open(f"{retrained_model_name}.pkl", 'wb') as f:
                                                pickle.dump(retrained_model, f)
                                            
                                            # Ajouter au session_state
                                            st.session_state.custom_models[retrained_model_name] = retrained_model
                                            
                                            # Combiner les statistiques
                                            combined_stats = {
                                                **stats,
                                                'preprocessing_stats': preprocess_stats,
                                                'base_dataset_size': len(base_dataset),
                                                'uploaded_dataset_size': len(new_data),
                                                'final_dataset_size': len(scaled_data)
                                            }
                                            
                                            # Sauvegarder les m√©triques
                                            save_success = save_retrained_model_metrics(
                                                retrained_model_name, metrics, selected_retrain_model, combined_stats
                                            )
                                            
                                            if save_success:
                                                st.success(f"üíæ Mod√®le r√©entra√Æn√© sauvegard√© sous '{retrained_model_name}.pkl'")
                                                
                                                # Afficher les m√©triques compl√®tes
                                                display_retrained_metrics_from_json({
                                                    'model_name': retrained_model_name,
                                                    'original_model': selected_retrain_model,
                                                    **metrics,
                                                    'dataset_stats': combined_stats,
                                                    'retrain_time': pd.Timestamp.now().isoformat()
                                                }, retrained_model_name)
                                                
                                                st.balloons()
                                                st.markdown("---")
                                                st.success("üéâ R√©entra√Ænement termin√© avec succ√®s! Le mod√®le est maintenant disponible dans la liste des mod√®les.")
                                                st.info(f"üîç Le mod√®le '{retrained_model_name}' est maintenant s√©lectionnable dans l'onglet 'Pr√©diction'")
                                                
                                            
                                        except Exception as e:
                                            st.error(f"‚ùå Erreur lors de la sauvegarde: {e}")
                                
                                except Exception as e:
                                    st.error(f"‚ùå Erreur lors du r√©entra√Ænement: {str(e)}")
                
        except Exception as e:
            st.error(f"‚ùå Erreur lors du chargement du fichier: {str(e)}")
    else:
        if base_dataset is not None:
            st.info("‚ÑπÔ∏è Veuillez charger un fichier CSV pour commencer le r√©entra√Ænement")
    
    # Section de gestion des mod√®les r√©entra√Æn√©s
    st.markdown("---")
    st.subheader("üìã Gestion des Mod√®les R√©entra√Æn√©s")
    
    # Afficher les mod√®les r√©entra√Æn√©s existants
    retrained_models_list = [k for k in st.session_state.custom_models.keys() if is_retrained_model(k)]
    
    if retrained_models_list:
        st.write(f"**{len(retrained_models_list)} mod√®le(s) r√©entra√Æn√©(s) disponible(s):**")
        
        for model_name in retrained_models_list:
            col1, col2, col3 = st.columns([3, 2, 1])
            
            with col1:
                # Trouver les m√©triques correspondantes
                model_metrics = None
                for metric in load_retrained_model_metrics():
                    if metric['model_name'] == model_name:
                        model_metrics = metric
                        break
                
                if model_metrics:
                    original_model = model_metrics.get('original_model', 'Inconnu')
                    accuracy = model_metrics.get('accuracy', 0)
                    st.write(f"**{model_name}**")
                    st.write(f"Original: {original_model} | Accuracy: {accuracy:.4f}")
                else:
                    st.write(f"**{model_name}**")
            
            with col2:
                if model_metrics and 'retrain_time' in model_metrics:
                    try:
                        retrain_time = pd.to_datetime(model_metrics['retrain_time'])
                        st.write(f"R√©entra√Æn√© le: {retrain_time.strftime('%Y-%m-%d %H:%M')}")
                    except:
                        st.write("Date inconnue")
                else:
                    st.write("Date inconnue")
            
            with col3:
                if st.button("üóëÔ∏è", key=f"delete_retrained_{model_name}"):
                    success = delete_retrained_model(model_name)
                    if success:
                        st.success(f"‚úÖ Mod√®le r√©entra√Æn√© '{model_name}' supprim√©")
                        st.rerun()
        
        # Bouton pour supprimer tous les mod√®les r√©entra√Æn√©s
        if st.button("üóëÔ∏è Supprimer Tous les Mod√®les R√©entra√Æn√©s", type="secondary", use_container_width=True):
            delete_all_retrained_models()
            st.rerun()
    else:
        st.info("‚ÑπÔ∏è Aucun mod√®le r√©entra√Æn√© disponible")